{
 "cells": [
  {
   "cell_type": "code",
   "id": "1a88a67d",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL'] = 'True'\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'sklearn'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5f3a645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:32:55.535200Z",
     "start_time": "2025-06-22T10:32:54.316406Z"
    }
   },
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet as fp\n",
    "import time\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "keras.utils.set_random_seed(1)"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clip_to_image_size' from 'keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters' (C:\\Users\\arsen\\anaconda3\\envs\\keras_jax\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\image_preprocessing\\bounding_boxes\\converters.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Input, Dense, Flatten\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\keras_jax\\lib\\site-packages\\keras\\layers\\__init__.py:243\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_hue\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    238\u001B[0m     RandomHue \u001B[38;5;28;01mas\u001B[39;00m RandomHue,\n\u001B[0;32m    239\u001B[0m )\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_invert\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    241\u001B[0m     RandomInvert \u001B[38;5;28;01mas\u001B[39;00m RandomInvert,\n\u001B[0;32m    242\u001B[0m )\n\u001B[1;32m--> 243\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_perspective\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    244\u001B[0m     RandomPerspective \u001B[38;5;28;01mas\u001B[39;00m RandomPerspective,\n\u001B[0;32m    245\u001B[0m )\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_posterization\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    247\u001B[0m     RandomPosterization \u001B[38;5;28;01mas\u001B[39;00m RandomPosterization,\n\u001B[0;32m    248\u001B[0m )\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_rotation\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    250\u001B[0m     RandomRotation \u001B[38;5;28;01mas\u001B[39;00m RandomRotation,\n\u001B[0;32m    251\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\keras_jax\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\image_preprocessing\\random_perspective.py:5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi_export\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m keras_export\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_image_preprocessing_layer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     BaseImagePreprocessingLayer,\n\u001B[0;32m      4\u001B[0m )\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbounding_boxes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconverters\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     clip_to_image_size,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbounding_boxes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconverters\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     convert_format,\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mseed_generator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SeedGenerator\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'clip_to_image_size' from 'keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters' (C:\\Users\\arsen\\anaconda3\\envs\\keras_jax\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\image_preprocessing\\bounding_boxes\\converters.py)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sigtkan_manual import SigTKAN\n",
    "from tkan import TKAN"
   ],
   "id": "ac44c62eda3cf245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_parquet('data.parquet')\n",
    "df = df[(df.index >= pd.Timestamp('2020-01-01')) & (df.index < pd.Timestamp('2023-01-01'))]\n",
    "assets = ['BTC', 'ETH', 'ADA', 'XMR', 'EOS', 'MATIC', 'TRX', 'FTM', 'BNB', 'XLM', 'ENJ', 'CHZ', 'BUSD', 'ATOM', 'LINK', 'ETC', 'XRP', 'BCH', 'LTC']\n",
    "df = df[[c for c in df.columns if 'quote asset volume' in c and any(asset in c for asset in assets)]]\n",
    "df.columns = [c.replace(' quote asset volume', '') for c in df.columns]\n",
    "df = df.head(500)\n",
    "display(df)"
   ],
   "id": "893ce5f59d42781d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MinMaxScaler:\n",
    "    def __init__(self, feature_axis=None, minmax_range=(0, 1)):\n",
    "        \"\"\"\n",
    "        Initialize the MinMaxScaler.\n",
    "        Args:\n",
    "        feature_axis (int, optional): The axis that represents the feature dimension if applicable.\n",
    "                                      Use only for 3D data to specify which axis is the feature axis.\n",
    "                                      Default is None, automatically managed based on data dimensions.\n",
    "        \"\"\"\n",
    "        self.feature_axis = feature_axis\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "        self.scale_ = None\n",
    "        self.minmax_range = minmax_range # Default range for scaling (min, max)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the scaler to the data based on its dimensionality.\n",
    "        Args:\n",
    "        X (np.array): The data to fit the scaler on.\n",
    "        \"\"\"\n",
    "        if X.ndim == 3 and self.feature_axis is not None:  # 3D data\n",
    "            axis = tuple(i for i in range(X.ndim) if i != self.feature_axis)\n",
    "            self.min_ = np.min(X, axis=axis)\n",
    "            self.max_ = np.max(X, axis=axis)\n",
    "        elif X.ndim == 2:  # 2D data\n",
    "            self.min_ = np.min(X, axis=0)\n",
    "            self.max_ = np.max(X, axis=0)\n",
    "        elif X.ndim == 1:  # 1D data\n",
    "            self.min_ = np.min(X)\n",
    "            self.max_ = np.max(X)\n",
    "        else:\n",
    "            raise ValueError(\"Data must be 1D, 2D, or 3D.\")\n",
    "\n",
    "        self.scale_ = self.max_ - self.min_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data using the fitted scaler.\n",
    "        Args:\n",
    "        X (np.array): The data to transform.\n",
    "        Returns:\n",
    "        np.array: The scaled data.\n",
    "        \"\"\"\n",
    "        X_scaled = (X - self.min_) / self.scale_\n",
    "        X_scaled = X_scaled * (self.minmax_range[1] - self.minmax_range[0]) + self.minmax_range[0]\n",
    "        return X_scaled\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        Args:\n",
    "        X (np.array): The data to fit and transform.\n",
    "        Returns:\n",
    "        np.array: The scaled data.\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        Inverse transform the scaled data to original data.\n",
    "        Args:\n",
    "        X_scaled (np.array): The scaled data to inverse transform.\n",
    "        Returns:\n",
    "        np.array: The original data scale.\n",
    "        \"\"\"\n",
    "        X = (X_scaled - self.minmax_range[0]) / (self.minmax_range[1] - self.minmax_range[0])\n",
    "        X = X * self.scale_ + self.min_\n",
    "        return X\n",
    "\n",
    "def generate_data(df, sequence_length, n_ahead = 1):\n",
    "    #Case without known inputs\n",
    "    scaler_df = df.copy().shift(n_ahead).rolling(24 * 14).median()\n",
    "    tmp_df = df.copy() / scaler_df\n",
    "    tmp_df = tmp_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    scaler_df = scaler_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    def prepare_sequences(df, scaler_df, n_history, n_future):\n",
    "        X, y, y_scaler = [], [], []\n",
    "        num_features = df.shape[1]\n",
    "        \n",
    "        # Iterate through the DataFrame to create sequences\n",
    "        for i in range(n_history, len(df) - n_future + 1):\n",
    "            # Extract the sequence of past observations\n",
    "            X.append(df.iloc[i - n_history:i].values)\n",
    "            # Extract the future values of the first column\n",
    "            y.append(df.iloc[i:i + n_future,0:1].values)\n",
    "            y_scaler.append(scaler_df.iloc[i:i + n_future,0:1].values)\n",
    "        \n",
    "        X, y, y_scaler = np.array(X), np.array(y), np.array(y_scaler)\n",
    "        return X, y, y_scaler\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X, y, y_scaler = prepare_sequences(tmp_df, scaler_df, sequence_length, n_ahead)\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_test_separation = int(len(X) * 0.8)\n",
    "    X_train_unscaled, X_test_unscaled = X[:train_test_separation], X[train_test_separation:]\n",
    "    y_train_unscaled, y_test_unscaled = y[:train_test_separation], y[train_test_separation:]\n",
    "    y_scaler_train, y_scaler_test = y_scaler[:train_test_separation], y_scaler[train_test_separation:]\n",
    "    \n",
    "    # Generate the data\n",
    "    X_scaler = MinMaxScaler(feature_axis=2)\n",
    "    X_train = X_scaler.fit_transform(X_train_unscaled)\n",
    "    X_test = X_scaler.transform(X_test_unscaled)\n",
    "    \n",
    "    y_scaler = MinMaxScaler(feature_axis=2)\n",
    "    y_train = y_scaler.fit_transform(y_train_unscaled)\n",
    "    y_test = y_scaler.transform(y_test_unscaled)\n",
    "    \n",
    "    y_train = y_train.reshape(y_train.shape[0], -1) \n",
    "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "    return X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test"
   ],
   "id": "50bf6e85d60bd1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "early_stopping_callback = lambda : tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00001,    \n",
    "    patience=6,         \n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=3, \n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "lr_callback = lambda : tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.25,\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.0001,  \n",
    "    min_lr=0.000025,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callbacks = lambda : [early_stopping_callback(), lr_callback(), tf.keras.callbacks.TerminateOnNaN()]"
   ],
   "id": "6be15d94c5b31234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tkan_config(return_sequences):\n",
    "    return {\n",
    "        \"units\": 100,\n",
    "        \"dropout\": 0.0,\n",
    "        \"sub_kan_output_dim\": 20,\n",
    "        \"sub_kan_input_dim\": 20,\n",
    "        \"return_sequences\": return_sequences\n",
    "    }\n",
    "\n",
    "n_aheads = [1]\n",
    "models = [\"TKAN\", \"SigTKAN\"]\n",
    "N_MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "callbacks = lambda : [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n",
    "\n",
    "results = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "results_rmse = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "time_results = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "all_errors = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "\n",
    "for n_ahead in n_aheads:\n",
    "    sequence_length = max(45, 5 * n_ahead)\n",
    "    X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data(df, sequence_length, n_ahead)\n",
    "    \n",
    "    for model_id in models:\n",
    "        for run in range(10):\n",
    "            if model_id == 'TKAN':\n",
    "                model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    TKAN(**tkan_config(return_sequences=True)),\n",
    "                    TKAN(**tkan_config(return_sequences=False)),\n",
    "                    Dense(units=n_ahead, activation='linear')\n",
    "                ], name=model_id)\n",
    "\n",
    "            elif model_id == 'SigTKAN':\n",
    "                model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    SigTKAN(sig_level=2, **tkan_config(return_sequences=True)),\n",
    "                    SigTKAN(sig_level=2, **tkan_config(return_sequences=False)),\n",
    "                    Dense(units=n_ahead, activation='linear')\n",
    "                ], name=model_id)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_id: {model_id}\")\n",
    "\n",
    "            optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error', jit_compile=True)\n",
    "            if run == 0:\n",
    "                model.summary()\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=N_MAX_EPOCHS, validation_split=0.2, callbacks=callbacks(), shuffle=True, verbose=False)\n",
    "            end_time = time.time()\n",
    "            time_results[model_id][n_ahead].append(end_time - start_time)\n",
    "\n",
    "            preds = model.predict(X_test, verbose=False)\n",
    "            r2 = r2_score(y_true=y_test, y_pred=preds)\n",
    "            rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n",
    "            \n",
    "            errors = preds - y_test\n",
    "            all_errors[model_id][n_ahead].append(errors)\n",
    "\n",
    "            print(end_time - start_time, r2)\n",
    "\n",
    "            results[model_id][n_ahead].append(r2)\n",
    "            results_rmse[model_id][n_ahead].append(rmse)\n",
    "\n",
    "            del model\n",
    "            del optimizer\n",
    "\n",
    "\n",
    "print('R2 scores - Means:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.mean(results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results}))\n",
    "\n",
    "print('RMSE scores - Means:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.mean(results_rmse[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results_rmse}))\n",
    "\n",
    "print('R2 scores - Std:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.std(results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results}))\n",
    "\n",
    "print('RMSE scores - Std:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.std(results_rmse[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results_rmse}))\n",
    "\n",
    "print('Training Times - Means:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.mean(time_results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in time_results}))\n",
    "\n",
    "print('Training Times - Std:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.std(time_results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in time_results}))"
   ],
   "id": "1fe592255fb35c9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Préparer all_errors pour le plot\n",
    "all_errors_plot = {}\n",
    "all_errors_plot['TKAN'] = np.mean(all_errors['TKAN'][1], axis=0)\n",
    "all_errors_plot['SigTKAN'] = np.mean(all_errors['SigTKAN'][1], axis=0)    \n",
    "\n",
    "model_types = ['TKAN', 'SigTKAN']\n",
    "colors = ['#1f77b4', '#2ca02c'] \n",
    "\n",
    "for model_type, color in zip(model_types, colors):\n",
    "    y_pred = all_errors_plot[model_type] + y_test\n",
    "    r2 = r2_score(y_true=y_test.flatten(), y_pred=y_pred.flatten())\n",
    "    plt.plot(np.mean(all_errors_plot[model_type]**2, axis=0), label=f'{model_type}: R²={round(r2,4)}', color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Model and errors based on number of steps forward')\n",
    "plt.xlabel('number of steps forward')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ],
   "id": "e01e65de407d83bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
