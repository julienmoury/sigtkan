{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib tensorflow tkan==0.3.0 sigkan==0.1.5 tkat==0.1.1 scikit-learn pyarrow keras-efficient-kan keras-sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Flatten, Input\n",
    "\n",
    "from tkan import TKAN\n",
    "from tkat import TKAT\n",
    "from sigkan import SigKAN\n",
    "from sigtkan import SigTKAN\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeddc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    def __init__(self, feature_axis=None, minmax_range=(0, 1)):\n",
    "        \"\"\"\n",
    "        Initialize the MinMaxScaler.\n",
    "        Args:\n",
    "        feature_axis (int, optional): The axis that represents the feature dimension if applicable.\n",
    "                                      Use only for 3D data to specify which axis is the feature axis.\n",
    "                                      Default is None, automatically managed based on data dimensions.\n",
    "        \"\"\"\n",
    "        self.feature_axis = feature_axis\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "        self.scale_ = None\n",
    "        self.minmax_range = minmax_range # Default range for scaling (min, max)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the scaler to the data based on its dimensionality.\n",
    "        Args:\n",
    "        X (np.array): The data to fit the scaler on.\n",
    "        \"\"\"\n",
    "        if X.ndim == 3 and self.feature_axis is not None:  # 3D data\n",
    "            axis = tuple(i for i in range(X.ndim) if i != self.feature_axis)\n",
    "            self.min_ = np.min(X, axis=axis)\n",
    "            self.max_ = np.max(X, axis=axis)\n",
    "        elif X.ndim == 2:  # 2D data\n",
    "            self.min_ = np.min(X, axis=0)\n",
    "            self.max_ = np.max(X, axis=0)\n",
    "        elif X.ndim == 1:  # 1D data\n",
    "            self.min_ = np.min(X)\n",
    "            self.max_ = np.max(X)\n",
    "        else:\n",
    "            raise ValueError(\"Data must be 1D, 2D, or 3D.\")\n",
    "\n",
    "        self.scale_ = self.max_ - self.min_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data using the fitted scaler.\n",
    "        Args:\n",
    "        X (np.array): The data to transform.\n",
    "        Returns:\n",
    "        np.array: The scaled data.\n",
    "        \"\"\"\n",
    "        X_scaled = (X - self.min_) / self.scale_\n",
    "        X_scaled = X_scaled * (self.minmax_range[1] - self.minmax_range[0]) + self.minmax_range[0]\n",
    "        return X_scaled\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        Args:\n",
    "        X (np.array): The data to fit and transform.\n",
    "        Returns:\n",
    "        np.array: The scaled data.\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        Inverse transform the scaled data to original data.\n",
    "        Args:\n",
    "        X_scaled (np.array): The scaled data to inverse transform.\n",
    "        Returns:\n",
    "        np.array: The original data scale.\n",
    "        \"\"\"\n",
    "        X = (X_scaled - self.minmax_range[0]) / (self.minmax_range[1] - self.minmax_range[0])\n",
    "        X = X * self.scale_ + self.min_\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f35d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/workspace/data.parquet')\n",
    "df = df[(df.index >= pd.Timestamp('2020-01-01')) & (df.index < pd.Timestamp('2023-01-01'))]\n",
    "assets = ['BTC', 'ETH', 'ADA', 'XMR', 'EOS', 'MATIC', 'TRX', 'FTM', 'BNB', 'XLM', 'ENJ', 'CHZ', 'BUSD', 'ATOM', 'LINK', 'ETC', 'XRP', 'BCH', 'LTC']\n",
    "df = df[[c for c in df.columns if 'quote asset volume' in c and any(asset in c for asset in assets)]]\n",
    "df.columns = [c.replace(' quote asset volume', '') for c in df.columns]\n",
    "known_input_df = pd.DataFrame(index=df.index, data=np.array([df.reset_index()['group'].apply(lambda x: (x.hour)).values, df.reset_index()['group'].apply(lambda x: (x.dayofweek)).values]).T, columns = ['hour', 'dayofweek'])\n",
    "display(df)\n",
    "display(known_input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MAX_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "early_stopping_callback = lambda : tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00001,\n",
    "    patience=6,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=6,\n",
    ")\n",
    "lr_callback = lambda : tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.25,\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.00001,\n",
    "    min_lr=0.000025,\n",
    "    verbose=0,\n",
    ")\n",
    "callbacks = lambda : [early_stopping_callback(), lr_callback(), tf.keras.callbacks.TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(df, sequence_length, n_ahead):\n",
    "    #Case without known inputs\n",
    "    scaler_df = df.copy().shift(n_ahead).rolling(24 * 14).median()\n",
    "    tmp_df = df.copy() / scaler_df\n",
    "    tmp_df = tmp_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    scaler_df = scaler_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    def prepare_sequences(df, scaler_df, n_history, n_future):\n",
    "        X, y, y_scaler = [], [], []\n",
    "        num_features = df.shape[1]\n",
    "        \n",
    "        # Iterate through the DataFrame to create sequences\n",
    "        for i in range(n_history, len(df) - n_future + 1):\n",
    "            # Extract the sequence of past observations\n",
    "            X.append(df.iloc[i - n_history:i].values)\n",
    "            # Extract the future values of the first column\n",
    "            y.append(df.iloc[i:i + n_future,0:1].values)\n",
    "            y_scaler.append(scaler_df.iloc[i:i + n_future,0:1].values)\n",
    "        \n",
    "        X, y, y_scaler = np.array(X), np.array(y), np.array(y_scaler)\n",
    "        return X, y, y_scaler\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X, y, y_scaler = prepare_sequences(tmp_df, scaler_df, sequence_length, n_ahead)\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_test_separation = int(len(X) * 0.8)\n",
    "    X_train_unscaled, X_test_unscaled = X[:train_test_separation], X[train_test_separation:]\n",
    "    y_train_unscaled, y_test_unscaled = y[:train_test_separation], y[train_test_separation:]\n",
    "    y_scaler_train, y_scaler_test = y_scaler[:train_test_separation], y_scaler[train_test_separation:]\n",
    "    \n",
    "    # Generate the data\n",
    "    X_scaler = MinMaxScaler(feature_axis=2)\n",
    "    X_train = X_scaler.fit_transform(X_train_unscaled)\n",
    "    X_test = X_scaler.transform(X_test_unscaled)\n",
    "    \n",
    "    y_scaler = MinMaxScaler(feature_axis=2)\n",
    "    y_train = y_scaler.fit_transform(y_train_unscaled)\n",
    "    y_test = y_scaler.transform(y_test_unscaled)\n",
    "    \n",
    "    y_train = y_train.reshape(y_train.shape[0], -1) \n",
    "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "    return X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test\n",
    "\n",
    "def generate_data_w_known_inputs(df, known_input_df, sequence_length, n_ahead):\n",
    "    #Case without known inputs - fill with 0 the unknown features future values in X\n",
    "    scaler_df = df.copy().shift(n_ahead).rolling(24 * 14).median()\n",
    "    tmp_df = df.copy() / scaler_df\n",
    "    tmp_df = tmp_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    scaler_df = scaler_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    tmp_known_input_df = known_input_df.iloc[24 * 14 + n_ahead:].copy()\n",
    "    def prepare_sequences(df, known_input_df, scaler_df, n_history, n_future):\n",
    "        Xu, Xk, y, y_scaler = [], [], [], []\n",
    "        num_features = df.shape[1]\n",
    "        \n",
    "        # Iterate through the DataFrame to create sequences\n",
    "        for i in range(n_history, len(df) - n_future + 1):\n",
    "            # Extract the sequence of past observations\n",
    "            Xu.append(np.concatenate((df.iloc[i - n_history:i].values, np.zeros((n_future, df.shape[1]))), axis=0))\n",
    "            Xk.append(known_input_df.iloc[i - n_history:i+n_future].values)\n",
    "            # Extract the future values of the first column\n",
    "            y.append(df.iloc[i:i + n_future,0:1].values)\n",
    "            y_scaler.append(scaler_df.iloc[i:i + n_future,0:1].values)\n",
    "        \n",
    "        Xu, Xk, y, y_scaler = np.array(Xu), np.array(Xk), np.array(y), np.array(y_scaler)\n",
    "        return Xu, Xk, y, y_scaler\n",
    "    \n",
    "    # Prepare sequences\n",
    "    Xu, Xk, y, y_scaler = prepare_sequences(tmp_df, tmp_known_input_df, scaler_df, sequence_length, n_ahead)\n",
    "\n",
    "    X = np.concatenate((Xu, Xk), axis=-1)\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_test_separation = int(len(X) * 0.8)\n",
    "    X_train_unscaled, X_test_unscaled = X[:train_test_separation], X[train_test_separation:]\n",
    "    y_train_unscaled, y_test_unscaled = y[:train_test_separation], y[train_test_separation:]\n",
    "    y_scaler_train, y_scaler_test = y_scaler[:train_test_separation], y_scaler[train_test_separation:]\n",
    "    \n",
    "    # Generate the data\n",
    "    X_scaler = MinMaxScaler(feature_axis=2)\n",
    "    X_train = X_scaler.fit_transform(X_train_unscaled)\n",
    "    X_test = X_scaler.transform(X_test_unscaled)\n",
    "    \n",
    "    y_scaler = MinMaxScaler(feature_axis=2)\n",
    "    y_train = y_scaler.fit_transform(y_train_unscaled)\n",
    "    y_test = y_scaler.transform(y_test_unscaled)\n",
    "    \n",
    "    y_train = y_train.reshape(y_train.shape[0], -1) \n",
    "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "    return X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead = 30\n",
    "sequence_length = 5 * n_ahead\n",
    "\n",
    "X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data(df, sequence_length, n_ahead)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371ed8c",
   "metadata": {},
   "source": [
    "# SigTKAN usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0912e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unknow_features = len(assets)\n",
    "num_know_features = X_train.shape[2] - num_unknow_features\n",
    "\n",
    "# Define SigTKAN model with signature level and KAN configurations\n",
    "model = Sequential([\n",
    "    Input(shape=X_train.shape[1:]),\n",
    "    SigTKAN(\n",
    "        units=100, \n",
    "        sig_level=2,  # Signature truncation level\n",
    "        sub_kan_configs=[{'grid_size': 3} for _ in range(4)],  # KAN layer configurations\n",
    "        sub_kan_output_dim=20,\n",
    "        sub_kan_input_dim=X_train.shape[2],\n",
    "        dropout=0.1,\n",
    "        return_sequences=False\n",
    "    ),\n",
    "    Dense(100, 'relu'),\n",
    "    Dense(units=n_ahead, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', jit_compile=False)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=N_MAX_EPOCHS, \n",
    "    validation_split=0.2, \n",
    "    callbacks=callbacks(), \n",
    "    shuffle=True, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "preds = model.predict(X_test).flatten()\n",
    "errors = preds - y_test.flatten()\n",
    "rmse = np.sqrt(np.mean(np.square(errors)))\n",
    "r2 = r2_score(y_true=y_test.flatten(), y_pred=preds)\n",
    "mae = np.mean(np.abs(errors))\n",
    "\n",
    "metrics_summary = f\"\"\"\n",
    "Model Type: SigTKAN\n",
    "------------------------------------\n",
    "Root Mean Squared Error (RMSE): {rmse:.4f}\n",
    "R-squared (R²) Score: {r2:.4f}\n",
    "Mean Absolute Error (MAE): {mae:.4f}\n",
    "\"\"\"\n",
    "print(metrics_summary)\n",
    "\n",
    "all_errors = {}\n",
    "preds = model.predict(X_test)\n",
    "errors = preds - y_test\n",
    "all_errors['SigTKAN'] = errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1b6dd",
   "metadata": {},
   "source": [
    "## Other example comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3622539",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['SigKAN', 'TKAT', 'TKAN', 'MLP', 'GRU', 'LSTM']\n",
    "\n",
    "for model_type in models:\n",
    "\n",
    "    if model_type == \"TKAT\":\n",
    "        X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data_w_known_inputs(df, known_input_df, sequence_length, n_ahead)\n",
    "    else:\n",
    "        X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data(df, sequence_length, n_ahead)\n",
    "\n",
    "    if model_type == \"TKAT\":\n",
    "        num_unknow_features = len(assets)\n",
    "        num_know_features = X_train.shape[2] - num_unknow_features\n",
    "        model = TKAT(sequence_length, num_unknow_features, num_know_features, 1, 100, 4, n_ahead, use_tkan=True)\n",
    "    elif model_type == 'SigKAN':\n",
    "        model = Sequential([\n",
    "            Input(shape=X_train.shape[1:]),\n",
    "            SigKAN(100, 2, dropout=0.),\n",
    "            Flatten(),\n",
    "            Dense(100, 'relu'),\n",
    "            Dense(units=n_ahead, activation='linear')\n",
    "        ])\n",
    "    elif 'TKAN' in model_type:\n",
    "        model = Sequential([\n",
    "            Input(shape=X_train.shape[1:]),\n",
    "            TKAN(100, tkan_activations=[{'grid_size': 3} for i in range(5)], sub_kan_output_dim=20, sub_kan_input_dim=1, return_sequences=True),\n",
    "            TKAN(100, tkan_activations=[{'grid_size': 3} for i in range(5)], sub_kan_output_dim=20, sub_kan_input_dim=1, return_sequences=False),\n",
    "            Dense(units=n_ahead, activation='linear')\n",
    "        ])\n",
    "    elif 'GRU' in model_type:\n",
    "        model = Sequential([\n",
    "            Input(shape=X_train.shape[1:]),\n",
    "            GRU(100, return_sequences=True),\n",
    "            GRU(100, return_sequences=False),\n",
    "            Dense(units=n_ahead, activation='linear')\n",
    "        ])\n",
    "    elif 'LSTM' in model_type:\n",
    "        model = Sequential([\n",
    "            Input(shape=X_train.shape[1:]),\n",
    "            LSTM(100, return_sequences=True),\n",
    "            LSTM(100, return_sequences=False),\n",
    "            Dense(units=n_ahead, activation='linear')\n",
    "        ])\n",
    "    elif 'MLP' in model_type:\n",
    "        model = Sequential([\n",
    "            Input(shape=X_train.shape[1:]),\n",
    "            Flatten(),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(units=n_ahead, activation='linear')\n",
    "        ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=N_MAX_EPOCHS, validation_split=0.2, callbacks=callbacks(), shuffle=True, verbose=False)\n",
    "    preds = model.predict(X_test)\n",
    "    errors = preds - y_test\n",
    "    all_errors[model_type] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['SigTKAN', 'SigKAN', 'TKAT', 'TKAN', 'MLP', 'GRU', 'LSTM']\n",
    "grey_shades = ['#FF0000', '#252525', '#404040', '#525252', '#737373', '#969696', '#bdbdbd']  # Red for SigTKAN, then darker to lighter\n",
    "\n",
    "for model_type, color in zip(model_types, grey_shades):\n",
    "    if model_type in all_errors:\n",
    "        y_pred = all_errors[model_type] + y_test\n",
    "        r2 = r2_score(y_true=y_test.flatten(), y_pred=y_pred.flatten())\n",
    "        plt.plot(np.mean(all_errors[model_type]**2, axis=0), label=f'{model_type}: R2={round(r2,4)}', color=color, linewidth=2 if model_type == 'SigTKAN' else 1)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Model comparison with SigTKAN - Errors based on number of steps forward')\n",
    "plt.xlabel('Number of steps forward')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('model_and_errors_sigtkan.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a96bec",
   "metadata": {},
   "source": [
    "## SigTKAN Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SigTKAN specific features\n",
    "print(\"SigTKAN Architecture Benefits:\")\n",
    "print(\"1. Signature Transform: Captures path-dependent features from sequential data\")\n",
    "print(\"2. Temporal KAN: Adaptive activation functions that learn optimal transformations\")\n",
    "print(\"3. Recurrent Structure: Maintains temporal dependencies while processing signatures\")\n",
    "print(\"4. Multi-scale Processing: Multiple sub-KAN layers for different feature aspects\")\n",
    "\n",
    "# Compare computational complexity\n",
    "print(\"\\nModel Complexity Comparison:\")\n",
    "for model_type in ['SigTKAN', 'SigKAN', 'TKAN', 'GRU', 'LSTM']:\n",
    "    if model_type in all_errors:\n",
    "        mse = np.mean(all_errors[model_type]**2)\n",
    "        r2_val = r2_score(y_true=y_test.flatten(), y_pred=(all_errors[model_type] + y_test).flatten())\n",
    "        print(f\"{model_type:>8}: MSE={mse:.6f}, R²={r2_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different SigTKAN configurations\n",
    "print(\"Testing SigTKAN with different signature levels...\")\n",
    "\n",
    "sigtkan_configs = [\n",
    "    {'sig_level': 1, 'label': 'SigTKAN (sig_level=1)'},\n",
    "    {'sig_level': 2, 'label': 'SigTKAN (sig_level=2)'},\n",
    "    {'sig_level': 3, 'label': 'SigTKAN (sig_level=3)'},\n",
    "]\n",
    "\n",
    "sigtkan_results = {}\n",
    "\n",
    "for config in sigtkan_configs:\n",
    "    print(f\"\\nTraining {config['label']}...\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=X_train.shape[1:]),\n",
    "        SigTKAN(\n",
    "            units=100,\n",
    "            sig_level=config['sig_level'],\n",
    "            sub_kan_configs=[{'grid_size': 3} for _ in range(4)],\n",
    "            sub_kan_output_dim=20,\n",
    "            sub_kan_input_dim=X_train.shape[2],\n",
    "            dropout=0.1,\n",
    "            return_sequences=False\n",
    "        ),\n",
    "        Dense(100, 'relu'),\n",
    "        Dense(units=n_ahead, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', jit_compile=False)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=N_MAX_EPOCHS,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks(),\n",
    "        shuffle=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    errors = preds - y_test\n",
    "    r2_val = r2_score(y_true=y_test.flatten(), y_pred=preds.flatten())\n",
    "    mse = np.mean(errors**2)\n",
    "    \n",
    "    sigtkan_results[config['label']] = {\n",
    "        'errors': errors,\n",
    "        'r2': r2_val,\n",
    "        'mse': mse\n",
    "    }\n",
    "    \n",
    "    print(f\"R² Score: {r2_val:.4f}, MSE: {mse:.6f}\")\n",
    "\n",
    "# Plot comparison of different SigTKAN configurations\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['#FF0000', '#FF6666', '#FF9999']\n",
    "\n",
    "for i, (label, results) in enumerate(sigtkan_results.items()):\n",
    "    plt.plot(\n",
    "        np.mean(results['errors']**2, axis=0),\n",
    "        label=f\"{label}: R²={results['r2']:.4f}\",\n",
    "        color=colors[i],\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.title('SigTKAN Performance with Different Signature Levels')\n",
    "plt.xlabel('Number of steps forward')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('sigtkan_signature_levels_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nBest performing models:\")\n",
    "performance_data = []\n",
    "\n",
    "# Add main comparison models\n",
    "for model_type in ['SigTKAN', 'SigKAN', 'TKAT', 'TKAN', 'MLP', 'GRU', 'LSTM']:\n",
    "    if model_type in all_errors:\n",
    "        y_pred = all_errors[model_type] + y_test\n",
    "        r2_val = r2_score(y_true=y_test.flatten(), y_pred=y_pred.flatten())\n",
    "        mse = np.mean(all_errors[model_type]**2)\n",
    "        performance_data.append((model_type, r2_val, mse))\n",
    "\n",
    "# Add SigTKAN variants\n",
    "for label, results in sigtkan_results.items():\n",
    "    performance_data.append((label, results['r2'], results['mse']))\n",
    "\n",
    "# Sort by R² score (descending)\n",
    "performance_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (model, r2_val, mse) in enumerate(performance_data, 1):\n",
    "    print(f\"{i:2d}. {model:<25} | R²: {r2_val:7.4f} | MSE: {mse:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"SigTKAN demonstrates {'superior' if performance_data[0][0].startswith('SigTKAN') else 'competitive'} performance\")\n",
    "print(\"combining signature transforms with temporal KAN processing.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
