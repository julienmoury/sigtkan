{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 🧹 Nettoyage et setup initial\n",
        "import os, sys, shutil\n",
        "\n",
        "# 1. Revenir à la racine\n",
        "os.chdir(\"/content\")\n",
        "print(\"📍 Répertoire actuel :\", os.getcwd())\n",
        "\n",
        "# 2. Supprimer toute copie existante de SigKAN\n",
        "if os.path.exists(\"sigtkan\"):\n",
        "    shutil.rmtree(\"sigtkan\")\n",
        "    print(\"🧹 Dossier sigtkan supprimé\")\n",
        "\n",
        "# 3. Cloner le dépôt GitHub\n",
        "!git clone https://github.com/julienmoury/sigtkan.git\n",
        "%cd TKAN\n",
        "\n",
        "# 4. Ajouter le projet au PYTHONPATH\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# 5. Installer les dépendances\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    %pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"⚠️ Pas de requirements.txt trouvé\")\n",
        "\n",
        "# 6. Afficher où on est et ce qu’on a\n",
        "print(\"📂 Répertoire courant :\", os.getcwd())\n",
        "print(\"📁 Contenu :\", os.listdir())"
      ],
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tPW-12JPt7K",
        "outputId": "2814542f-ee89-402f-cffb-ec9ef76b1c2f"
=======
        "id": "0tPW-12JPt7K",
        "outputId": "7bdc4f40-8fca-4077-f079-00531812cb89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
>>>>>>> 87fa4aa9c255fdad68b329eb504a6ee449af3989
      },
      "id": "0tPW-12JPt7K",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📍 Répertoire actuel : /content\n",
<<<<<<< HEAD
            "🧹 Dossier sigtkan supprimé\n",
            "Cloning into 'sigtkan'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 18 (delta 2), reused 0 (delta 0), pack-reused 11 (from 2)\u001b[K\n",
            "Receiving objects: 100% (18/18), 56.94 MiB | 27.82 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
=======
            "Cloning into 'sigtkan'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 12 (delta 0), reused 0 (delta 0), pack-reused 11 (from 2)\u001b[K\n",
            "Receiving objects: 100% (12/12), 56.93 MiB | 9.14 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (8/8), done.\n",
>>>>>>> 87fa4aa9c255fdad68b329eb504a6ee449af3989
            "[Errno 2] No such file or directory: 'TKAN'\n",
            "/content\n",
            "⚠️ Pas de requirements.txt trouvé\n",
            "📂 Répertoire courant : /content\n",
            "📁 Contenu : ['.config', 'sigtkan', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib tensorflow tkan==0.3.0 sigkan==0.1.5 tkat==0.1.1 scikit-learn pyarrow keras-efficient-kan keras-sig"
      ],
      "metadata": {
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M83cFF0Puu8",
        "outputId": "c9e367fd-c5d8-46ba-bd51-36ea03cefa3a"
      },
      "id": "_M83cFF0Puu8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tkan==0.3.0 in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: sigkan==0.1.5 in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: tkat==0.1.1 in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: keras-efficient-kan in /usr/local/lib/python3.11/dist-packages (0.1.8)\n",
            "Requirement already satisfied: keras-sig in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: iisignature_tensorflow_2<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from sigkan==0.1.5) (0.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jaxtyping<0.3.0,>=0.2.36 in /usr/local/lib/python3.11/dist-packages (from keras-sig) (0.2.38)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: iisignature>=0.24 in /usr/local/lib/python3.11/dist-packages (from iisignature_tensorflow_2<0.2.0,>=0.1.0->sigkan==0.1.5) (0.24)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping<0.3.0,>=0.2.36->keras-sig) (0.1.7)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2964404e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "2964404e",
        "outputId": "5615d397-3ee8-4ea2-dde8-0ca0421c685b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'grns'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-436113417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTKAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msigkan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSigKAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msigtkan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSigTKAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/sigtkan/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"SigTKAN: Signature-enhanced Temporal Kolmogorov-Arnold Networks\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msigtkan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSigTKAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigTKANCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.1.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/sigtkan/sigtkan.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_efficient_kan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKANLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_sig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSigLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgrns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGRKAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grns'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
=======
        "id": "_M83cFF0Puu8"
      },
      "id": "_M83cFF0Puu8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2964404e",
      "metadata": {
        "id": "2964404e"
      },
      "outputs": [],
>>>>>>> 87fa4aa9c255fdad68b329eb504a6ee449af3989
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, LSTM, Dense, Flatten, Input\n",
        "\n",
        "from tkan import TKAN\n",
        "from tkat import TKAT\n",
        "from sigkan import SigKAN\n",
        "from sigtkan import SigTKAN\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daeddc95",
      "metadata": {
        "id": "daeddc95"
      },
      "outputs": [],
      "source": [
        "class MinMaxScaler:\n",
        "    def __init__(self, feature_axis=None, minmax_range=(0, 1)):\n",
        "        \"\"\"\n",
        "        Initialize the MinMaxScaler.\n",
        "        Args:\n",
        "        feature_axis (int, optional): The axis that represents the feature dimension if applicable.\n",
        "                                      Use only for 3D data to specify which axis is the feature axis.\n",
        "                                      Default is None, automatically managed based on data dimensions.\n",
        "        \"\"\"\n",
        "        self.feature_axis = feature_axis\n",
        "        self.min_ = None\n",
        "        self.max_ = None\n",
        "        self.scale_ = None\n",
        "        self.minmax_range = minmax_range # Default range for scaling (min, max)\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit the scaler to the data based on its dimensionality.\n",
        "        Args:\n",
        "        X (np.array): The data to fit the scaler on.\n",
        "        \"\"\"\n",
        "        if X.ndim == 3 and self.feature_axis is not None:  # 3D data\n",
        "            axis = tuple(i for i in range(X.ndim) if i != self.feature_axis)\n",
        "            self.min_ = np.min(X, axis=axis)\n",
        "            self.max_ = np.max(X, axis=axis)\n",
        "        elif X.ndim == 2:  # 2D data\n",
        "            self.min_ = np.min(X, axis=0)\n",
        "            self.max_ = np.max(X, axis=0)\n",
        "        elif X.ndim == 1:  # 1D data\n",
        "            self.min_ = np.min(X)\n",
        "            self.max_ = np.max(X)\n",
        "        else:\n",
        "            raise ValueError(\"Data must be 1D, 2D, or 3D.\")\n",
        "\n",
        "        self.scale_ = self.max_ - self.min_\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the data using the fitted scaler.\n",
        "        Args:\n",
        "        X (np.array): The data to transform.\n",
        "        Returns:\n",
        "        np.array: The scaled data.\n",
        "        \"\"\"\n",
        "        X_scaled = (X - self.min_) / self.scale_\n",
        "        X_scaled = X_scaled * (self.minmax_range[1] - self.minmax_range[0]) + self.minmax_range[0]\n",
        "        return X_scaled\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Fit to data, then transform it.\n",
        "        Args:\n",
        "        X (np.array): The data to fit and transform.\n",
        "        Returns:\n",
        "        np.array: The scaled data.\n",
        "        \"\"\"\n",
        "        return self.fit(X).transform(X)\n",
        "\n",
        "    def inverse_transform(self, X_scaled):\n",
        "        \"\"\"\n",
        "        Inverse transform the scaled data to original data.\n",
        "        Args:\n",
        "        X_scaled (np.array): The scaled data to inverse transform.\n",
        "        Returns:\n",
        "        np.array: The original data scale.\n",
        "        \"\"\"\n",
        "        X = (X_scaled - self.minmax_range[0]) / (self.minmax_range[1] - self.minmax_range[0])\n",
        "        X = X * self.scale_ + self.min_\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f35d96",
      "metadata": {
        "id": "70f35d96"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet('/workspace/data.parquet')\n",
        "df = df[(df.index >= pd.Timestamp('2020-01-01')) & (df.index < pd.Timestamp('2023-01-01'))]\n",
        "assets = ['BTC', 'ETH', 'ADA', 'XMR', 'EOS', 'MATIC', 'TRX', 'FTM', 'BNB', 'XLM', 'ENJ', 'CHZ', 'BUSD', 'ATOM', 'LINK', 'ETC', 'XRP', 'BCH', 'LTC']\n",
        "df = df[[c for c in df.columns if 'quote asset volume' in c and any(asset in c for asset in assets)]]\n",
        "df.columns = [c.replace(' quote asset volume', '') for c in df.columns]\n",
        "known_input_df = pd.DataFrame(index=df.index, data=np.array([df.reset_index()['group'].apply(lambda x: (x.hour)).values, df.reset_index()['group'].apply(lambda x: (x.dayofweek)).values]).T, columns = ['hour', 'dayofweek'])\n",
        "display(df)\n",
        "display(known_input_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b434c6e2",
      "metadata": {
        "id": "b434c6e2"
      },
      "outputs": [],
      "source": [
        "N_MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "early_stopping_callback = lambda : tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.00001,\n",
        "    patience=6,\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=6,\n",
        ")\n",
        "lr_callback = lambda : tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.25,\n",
        "    patience=3,\n",
        "    mode=\"min\",\n",
        "    min_delta=0.00001,\n",
        "    min_lr=0.000025,\n",
        "    verbose=0,\n",
        ")\n",
        "callbacks = lambda : [early_stopping_callback(), lr_callback(), tf.keras.callbacks.TerminateOnNaN()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718a9cbb",
      "metadata": {
        "id": "718a9cbb"
      },
      "outputs": [],
      "source": [
        "def generate_data(df, sequence_length, n_ahead):\n",
        "    #Case without known inputs\n",
        "    scaler_df = df.copy().shift(n_ahead).rolling(24 * 14).median()\n",
        "    tmp_df = df.copy() / scaler_df\n",
        "    tmp_df = tmp_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
        "    scaler_df = scaler_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
        "    def prepare_sequences(df, scaler_df, n_history, n_future):\n",
        "        X, y, y_scaler = [], [], []\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        # Iterate through the DataFrame to create sequences\n",
        "        for i in range(n_history, len(df) - n_future + 1):\n",
        "            # Extract the sequence of past observations\n",
        "            X.append(df.iloc[i - n_history:i].values)\n",
        "            # Extract the future values of the first column\n",
        "            y.append(df.iloc[i:i + n_future,0:1].values)\n",
        "            y_scaler.append(scaler_df.iloc[i:i + n_future,0:1].values)\n",
        "\n",
        "        X, y, y_scaler = np.array(X), np.array(y), np.array(y_scaler)\n",
        "        return X, y, y_scaler\n",
        "\n",
        "    # Prepare sequences\n",
        "    X, y, y_scaler = prepare_sequences(tmp_df, scaler_df, sequence_length, n_ahead)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_test_separation = int(len(X) * 0.8)\n",
        "    X_train_unscaled, X_test_unscaled = X[:train_test_separation], X[train_test_separation:]\n",
        "    y_train_unscaled, y_test_unscaled = y[:train_test_separation], y[train_test_separation:]\n",
        "    y_scaler_train, y_scaler_test = y_scaler[:train_test_separation], y_scaler[train_test_separation:]\n",
        "\n",
        "    # Generate the data\n",
        "    X_scaler = MinMaxScaler(feature_axis=2)\n",
        "    X_train = X_scaler.fit_transform(X_train_unscaled)\n",
        "    X_test = X_scaler.transform(X_test_unscaled)\n",
        "\n",
        "    y_scaler = MinMaxScaler(feature_axis=2)\n",
        "    y_train = y_scaler.fit_transform(y_train_unscaled)\n",
        "    y_test = y_scaler.transform(y_test_unscaled)\n",
        "\n",
        "    y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "    return X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test\n",
        "\n",
        "def generate_data_w_known_inputs(df, known_input_df, sequence_length, n_ahead):\n",
        "    #Case without known inputs - fill with 0 the unknown features future values in X\n",
        "    scaler_df = df.copy().shift(n_ahead).rolling(24 * 14).median()\n",
        "    tmp_df = df.copy() / scaler_df\n",
        "    tmp_df = tmp_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
        "    scaler_df = scaler_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
        "    tmp_known_input_df = known_input_df.iloc[24 * 14 + n_ahead:].copy()\n",
        "    def prepare_sequences(df, known_input_df, scaler_df, n_history, n_future):\n",
        "        Xu, Xk, y, y_scaler = [], [], [], []\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        # Iterate through the DataFrame to create sequences\n",
        "        for i in range(n_history, len(df) - n_future + 1):\n",
        "            # Extract the sequence of past observations\n",
        "            Xu.append(np.concatenate((df.iloc[i - n_history:i].values, np.zeros((n_future, df.shape[1]))), axis=0))\n",
        "            Xk.append(known_input_df.iloc[i - n_history:i+n_future].values)\n",
        "            # Extract the future values of the first column\n",
        "            y.append(df.iloc[i:i + n_future,0:1].values)\n",
        "            y_scaler.append(scaler_df.iloc[i:i + n_future,0:1].values)\n",
        "\n",
        "        Xu, Xk, y, y_scaler = np.array(Xu), np.array(Xk), np.array(y), np.array(y_scaler)\n",
        "        return Xu, Xk, y, y_scaler\n",
        "\n",
        "    # Prepare sequences\n",
        "    Xu, Xk, y, y_scaler = prepare_sequences(tmp_df, tmp_known_input_df, scaler_df, sequence_length, n_ahead)\n",
        "\n",
        "    X = np.concatenate((Xu, Xk), axis=-1)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_test_separation = int(len(X) * 0.8)\n",
        "    X_train_unscaled, X_test_unscaled = X[:train_test_separation], X[train_test_separation:]\n",
        "    y_train_unscaled, y_test_unscaled = y[:train_test_separation], y[train_test_separation:]\n",
        "    y_scaler_train, y_scaler_test = y_scaler[:train_test_separation], y_scaler[train_test_separation:]\n",
        "\n",
        "    # Generate the data\n",
        "    X_scaler = MinMaxScaler(feature_axis=2)\n",
        "    X_train = X_scaler.fit_transform(X_train_unscaled)\n",
        "    X_test = X_scaler.transform(X_test_unscaled)\n",
        "\n",
        "    y_scaler = MinMaxScaler(feature_axis=2)\n",
        "    y_train = y_scaler.fit_transform(y_train_unscaled)\n",
        "    y_test = y_scaler.transform(y_test_unscaled)\n",
        "\n",
        "    y_train = y_train.reshape(y_train.shape[0], -1)\n",
        "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "    return X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73fc0e7",
      "metadata": {
        "id": "a73fc0e7"
      },
      "outputs": [],
      "source": [
        "n_ahead = 30\n",
        "sequence_length = 5 * n_ahead\n",
        "\n",
        "X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data(df, sequence_length, n_ahead)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9371ed8c",
      "metadata": {
        "id": "9371ed8c"
      },
      "source": [
        "# SigTKAN usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0912e5",
      "metadata": {
        "id": "5e0912e5"
      },
      "outputs": [],
      "source": [
        "num_unknow_features = len(assets)\n",
        "num_know_features = X_train.shape[2] - num_unknow_features\n",
        "\n",
        "# Define SigTKAN model with signature level and KAN configurations\n",
        "model = Sequential([\n",
        "    Input(shape=X_train.shape[1:]),\n",
        "    SigTKAN(\n",
        "        units=100,\n",
        "        sig_level=2,  # Signature truncation level\n",
        "        sub_kan_configs=[{'grid_size': 3} for _ in range(4)],  # KAN layer configurations\n",
        "        sub_kan_output_dim=20,\n",
        "        sub_kan_input_dim=X_train.shape[2],\n",
        "        dropout=0.1,\n",
        "        return_sequences=False\n",
        "    ),\n",
        "    Dense(100, 'relu'),\n",
        "    Dense(units=n_ahead, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', jit_compile=False)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=N_MAX_EPOCHS,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks(),\n",
        "    shuffle=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "preds = model.predict(X_test).flatten()\n",
        "errors = preds - y_test.flatten()\n",
        "rmse = np.sqrt(np.mean(np.square(errors)))\n",
        "r2 = r2_score(y_true=y_test.flatten(), y_pred=preds)\n",
        "mae = np.mean(np.abs(errors))\n",
        "\n",
        "metrics_summary = f\"\"\"\n",
        "Model Type: SigTKAN\n",
        "------------------------------------\n",
        "Root Mean Squared Error (RMSE): {rmse:.4f}\n",
        "R-squared (R²) Score: {r2:.4f}\n",
        "Mean Absolute Error (MAE): {mae:.4f}\n",
        "\"\"\"\n",
        "print(metrics_summary)\n",
        "\n",
        "all_errors = {}\n",
        "preds = model.predict(X_test)\n",
        "errors = preds - y_test\n",
        "all_errors['SigTKAN'] = errors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a1b6dd",
      "metadata": {
        "id": "52a1b6dd"
      },
      "source": [
        "## Other example comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3622539",
      "metadata": {
        "id": "e3622539"
      },
      "outputs": [],
      "source": [
        "models = ['SigKAN', 'TKAT', 'TKAN', 'MLP', 'GRU', 'LSTM']\n",
        "\n",
        "for model_type in models:\n",
        "\n",
        "    if model_type == \"TKAT\":\n",
        "        X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data_w_known_inputs(df, known_input_df, sequence_length, n_ahead)\n",
        "    else:\n",
        "        X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data(df, sequence_length, n_ahead)\n",
        "\n",
        "    if model_type == \"TKAT\":\n",
        "        num_unknow_features = len(assets)\n",
        "        num_know_features = X_train.shape[2] - num_unknow_features\n",
        "        model = TKAT(sequence_length, num_unknow_features, num_know_features, 1, 100, 4, n_ahead, use_tkan=True)\n",
        "    elif model_type == 'SigKAN':\n",
        "        model = Sequential([\n",
        "            Input(shape=X_train.shape[1:]),\n",
        "            SigKAN(100, 2, dropout=0.),\n",
        "            Flatten(),\n",
        "            Dense(100, 'relu'),\n",
        "            Dense(units=n_ahead, activation='linear')\n",
        "        ])\n",
        "    elif 'TKAN' in model_type:\n",
        "        model = Sequential([\n",
        "            Input(shape=X_train.shape[1:]),\n",
        "            TKAN(100, tkan_activations=[{'grid_size': 3} for i in range(5)], sub_kan_output_dim=20, sub_kan_input_dim=1, return_sequences=True),\n",
        "            TKAN(100, tkan_activations=[{'grid_size': 3} for i in range(5)], sub_kan_output_dim=20, sub_kan_input_dim=1, return_sequences=False),\n",
        "            Dense(units=n_ahead, activation='linear')\n",
        "        ])\n",
        "    elif 'GRU' in model_type:\n",
        "        model = Sequential([\n",
        "            Input(shape=X_train.shape[1:]),\n",
        "            GRU(100, return_sequences=True),\n",
        "            GRU(100, return_sequences=False),\n",
        "            Dense(units=n_ahead, activation='linear')\n",
        "        ])\n",
        "    elif 'LSTM' in model_type:\n",
        "        model = Sequential([\n",
        "            Input(shape=X_train.shape[1:]),\n",
        "            LSTM(100, return_sequences=True),\n",
        "            LSTM(100, return_sequences=False),\n",
        "            Dense(units=n_ahead, activation='linear')\n",
        "        ])\n",
        "    elif 'MLP' in model_type:\n",
        "        model = Sequential([\n",
        "            Input(shape=X_train.shape[1:]),\n",
        "            Flatten(),\n",
        "            Dense(100, activation='relu'),\n",
        "            Dense(100, activation='relu'),\n",
        "            Dense(units=n_ahead, activation='linear')\n",
        "        ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=N_MAX_EPOCHS, validation_split=0.2, callbacks=callbacks(), shuffle=True, verbose=False)\n",
        "    preds = model.predict(X_test)\n",
        "    errors = preds - y_test\n",
        "    all_errors[model_type] = errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922370b3",
      "metadata": {
        "id": "922370b3"
      },
      "outputs": [],
      "source": [
        "model_types = ['SigTKAN', 'SigKAN', 'TKAT', 'TKAN', 'MLP', 'GRU', 'LSTM']\n",
        "grey_shades = ['#FF0000', '#252525', '#404040', '#525252', '#737373', '#969696', '#bdbdbd']  # Red for SigTKAN, then darker to lighter\n",
        "\n",
        "for model_type, color in zip(model_types, grey_shades):\n",
        "    if model_type in all_errors:\n",
        "        y_pred = all_errors[model_type] + y_test\n",
        "        r2 = r2_score(y_true=y_test.flatten(), y_pred=y_pred.flatten())\n",
        "        plt.plot(np.mean(all_errors[model_type]**2, axis=0), label=f'{model_type}: R2={round(r2,4)}', color=color, linewidth=2 if model_type == 'SigTKAN' else 1)\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Model comparison with SigTKAN - Errors based on number of steps forward')\n",
        "plt.xlabel('Number of steps forward')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('model_and_errors_sigtkan.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a96bec",
      "metadata": {
        "id": "f5a96bec"
      },
      "source": [
        "## SigTKAN Architecture Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad4a239",
      "metadata": {
        "id": "4ad4a239"
      },
      "outputs": [],
      "source": [
        "# Analyze SigTKAN specific features\n",
        "print(\"SigTKAN Architecture Benefits:\")\n",
        "print(\"1. Signature Transform: Captures path-dependent features from sequential data\")\n",
        "print(\"2. Temporal KAN: Adaptive activation functions that learn optimal transformations\")\n",
        "print(\"3. Recurrent Structure: Maintains temporal dependencies while processing signatures\")\n",
        "print(\"4. Multi-scale Processing: Multiple sub-KAN layers for different feature aspects\")\n",
        "\n",
        "# Compare computational complexity\n",
        "print(\"\\nModel Complexity Comparison:\")\n",
        "for model_type in ['SigTKAN', 'SigKAN', 'TKAN', 'GRU', 'LSTM']:\n",
        "    if model_type in all_errors:\n",
        "        mse = np.mean(all_errors[model_type]**2)\n",
        "        r2_val = r2_score(y_true=y_test.flatten(), y_pred=(all_errors[model_type] + y_test).flatten())\n",
        "        print(f\"{model_type:>8}: MSE={mse:.6f}, R²={r2_val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42e15e2",
      "metadata": {
        "id": "e42e15e2"
      },
      "outputs": [],
      "source": [
        "# Test different SigTKAN configurations\n",
        "print(\"Testing SigTKAN with different signature levels...\")\n",
        "\n",
        "sigtkan_configs = [\n",
        "    {'sig_level': 1, 'label': 'SigTKAN (sig_level=1)'},\n",
        "    {'sig_level': 2, 'label': 'SigTKAN (sig_level=2)'},\n",
        "    {'sig_level': 3, 'label': 'SigTKAN (sig_level=3)'},\n",
        "]\n",
        "\n",
        "sigtkan_results = {}\n",
        "\n",
        "for config in sigtkan_configs:\n",
        "    print(f\"\\nTraining {config['label']}...\")\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=X_train.shape[1:]),\n",
        "        SigTKAN(\n",
        "            units=100,\n",
        "            sig_level=config['sig_level'],\n",
        "            sub_kan_configs=[{'grid_size': 3} for _ in range(4)],\n",
        "            sub_kan_output_dim=20,\n",
        "            sub_kan_input_dim=X_train.shape[2],\n",
        "            dropout=0.1,\n",
        "            return_sequences=False\n",
        "        ),\n",
        "        Dense(100, 'relu'),\n",
        "        Dense(units=n_ahead, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', jit_compile=False)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=N_MAX_EPOCHS,\n",
        "        validation_split=0.2,\n",
        "        callbacks=callbacks(),\n",
        "        shuffle=True,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "    errors = preds - y_test\n",
        "    r2_val = r2_score(y_true=y_test.flatten(), y_pred=preds.flatten())\n",
        "    mse = np.mean(errors**2)\n",
        "\n",
        "    sigtkan_results[config['label']] = {\n",
        "        'errors': errors,\n",
        "        'r2': r2_val,\n",
        "        'mse': mse\n",
        "    }\n",
        "\n",
        "    print(f\"R² Score: {r2_val:.4f}, MSE: {mse:.6f}\")\n",
        "\n",
        "# Plot comparison of different SigTKAN configurations\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['#FF0000', '#FF6666', '#FF9999']\n",
        "\n",
        "for i, (label, results) in enumerate(sigtkan_results.items()):\n",
        "    plt.plot(\n",
        "        np.mean(results['errors']**2, axis=0),\n",
        "        label=f\"{label}: R²={results['r2']:.4f}\",\n",
        "        color=colors[i],\n",
        "        linewidth=2\n",
        "    )\n",
        "\n",
        "plt.legend()\n",
        "plt.title('SigTKAN Performance with Different Signature Levels')\n",
        "plt.xlabel('Number of steps forward')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('sigtkan_signature_levels_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6877dfac",
      "metadata": {
        "id": "6877dfac"
      },
      "outputs": [],
      "source": [
        "# Final performance summary\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nBest performing models:\")\n",
        "performance_data = []\n",
        "\n",
        "# Add main comparison models\n",
        "for model_type in ['SigTKAN', 'SigKAN', 'TKAT', 'TKAN', 'MLP', 'GRU', 'LSTM']:\n",
        "    if model_type in all_errors:\n",
        "        y_pred = all_errors[model_type] + y_test\n",
        "        r2_val = r2_score(y_true=y_test.flatten(), y_pred=y_pred.flatten())\n",
        "        mse = np.mean(all_errors[model_type]**2)\n",
        "        performance_data.append((model_type, r2_val, mse))\n",
        "\n",
        "# Add SigTKAN variants\n",
        "for label, results in sigtkan_results.items():\n",
        "    performance_data.append((label, results['r2'], results['mse']))\n",
        "\n",
        "# Sort by R² score (descending)\n",
        "performance_data.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for i, (model, r2_val, mse) in enumerate(performance_data, 1):\n",
        "    print(f\"{i:2d}. {model:<25} | R²: {r2_val:7.4f} | MSE: {mse:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"SigTKAN demonstrates {'superior' if performance_data[0][0].startswith('SigTKAN') else 'competitive'} performance\")\n",
        "print(\"combining signature transforms with temporal KAN processing.\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}