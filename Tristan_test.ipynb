{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a88a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', 'sklearn'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL'] = 'True'\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'sklearn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81252f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f3a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet as fp\n",
    "import time\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sigtkan_Tristan import SigTKAN\n",
    "from tkan import TKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32a454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC</th>\n",
       "      <th>ADA</th>\n",
       "      <th>XMR</th>\n",
       "      <th>EOS</th>\n",
       "      <th>CHZ</th>\n",
       "      <th>MATIC</th>\n",
       "      <th>TRX</th>\n",
       "      <th>ENJ</th>\n",
       "      <th>FTM</th>\n",
       "      <th>BNB</th>\n",
       "      <th>XLM</th>\n",
       "      <th>BUSD</th>\n",
       "      <th>ATOM</th>\n",
       "      <th>LTC</th>\n",
       "      <th>LINK</th>\n",
       "      <th>ETC</th>\n",
       "      <th>ETH</th>\n",
       "      <th>XRP</th>\n",
       "      <th>BCH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>3.675857e+06</td>\n",
       "      <td>3.818918e+04</td>\n",
       "      <td>45395.983051</td>\n",
       "      <td>9.477858e+04</td>\n",
       "      <td>817.146319</td>\n",
       "      <td>31003.791035</td>\n",
       "      <td>4.819934e+05</td>\n",
       "      <td>15241.945783</td>\n",
       "      <td>1165.788613</td>\n",
       "      <td>8.498617e+05</td>\n",
       "      <td>9.460820e+03</td>\n",
       "      <td>1.352376e+04</td>\n",
       "      <td>3.198697e+04</td>\n",
       "      <td>1.165827e+05</td>\n",
       "      <td>2.428117e+04</td>\n",
       "      <td>5.648840e+04</td>\n",
       "      <td>1.000930e+06</td>\n",
       "      <td>2.579254e+05</td>\n",
       "      <td>1.782587e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:00:00</th>\n",
       "      <td>6.365953e+06</td>\n",
       "      <td>5.135701e+04</td>\n",
       "      <td>33483.951528</td>\n",
       "      <td>5.932921e+05</td>\n",
       "      <td>886.460339</td>\n",
       "      <td>84465.335718</td>\n",
       "      <td>5.336686e+05</td>\n",
       "      <td>11896.843688</td>\n",
       "      <td>413.844612</td>\n",
       "      <td>7.405759e+05</td>\n",
       "      <td>3.714191e+04</td>\n",
       "      <td>2.531605e+04</td>\n",
       "      <td>8.177767e+04</td>\n",
       "      <td>2.830715e+05</td>\n",
       "      <td>5.119098e+04</td>\n",
       "      <td>1.821021e+05</td>\n",
       "      <td>1.474278e+06</td>\n",
       "      <td>4.520609e+05</td>\n",
       "      <td>6.153210e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 02:00:00</th>\n",
       "      <td>4.736719e+06</td>\n",
       "      <td>3.616426e+04</td>\n",
       "      <td>15732.553860</td>\n",
       "      <td>2.667326e+05</td>\n",
       "      <td>1819.795050</td>\n",
       "      <td>113379.718506</td>\n",
       "      <td>3.870500e+05</td>\n",
       "      <td>30109.770521</td>\n",
       "      <td>3559.965968</td>\n",
       "      <td>1.039091e+06</td>\n",
       "      <td>1.687882e+04</td>\n",
       "      <td>1.390886e+04</td>\n",
       "      <td>1.957312e+05</td>\n",
       "      <td>2.402871e+05</td>\n",
       "      <td>2.872176e+04</td>\n",
       "      <td>1.340634e+05</td>\n",
       "      <td>9.940256e+05</td>\n",
       "      <td>4.414948e+05</td>\n",
       "      <td>2.215356e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:00</th>\n",
       "      <td>5.667367e+06</td>\n",
       "      <td>2.444995e+04</td>\n",
       "      <td>25751.054662</td>\n",
       "      <td>1.245166e+05</td>\n",
       "      <td>2979.655803</td>\n",
       "      <td>41771.707995</td>\n",
       "      <td>4.507721e+05</td>\n",
       "      <td>6732.833578</td>\n",
       "      <td>4076.415482</td>\n",
       "      <td>4.975018e+05</td>\n",
       "      <td>9.049223e+03</td>\n",
       "      <td>2.251969e+04</td>\n",
       "      <td>1.201133e+05</td>\n",
       "      <td>1.613043e+05</td>\n",
       "      <td>2.959622e+04</td>\n",
       "      <td>1.310942e+05</td>\n",
       "      <td>6.473610e+05</td>\n",
       "      <td>1.886061e+05</td>\n",
       "      <td>3.971860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>3.379094e+06</td>\n",
       "      <td>4.450267e+04</td>\n",
       "      <td>62955.628253</td>\n",
       "      <td>4.218197e+05</td>\n",
       "      <td>1023.388675</td>\n",
       "      <td>22254.756114</td>\n",
       "      <td>2.847890e+05</td>\n",
       "      <td>846.938455</td>\n",
       "      <td>633.367505</td>\n",
       "      <td>4.751285e+05</td>\n",
       "      <td>7.254260e+03</td>\n",
       "      <td>1.122460e+04</td>\n",
       "      <td>1.998917e+04</td>\n",
       "      <td>2.214516e+05</td>\n",
       "      <td>5.451437e+04</td>\n",
       "      <td>1.349371e+05</td>\n",
       "      <td>4.430067e+05</td>\n",
       "      <td>2.279373e+05</td>\n",
       "      <td>3.164991e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11 11:00:00</th>\n",
       "      <td>1.438900e+07</td>\n",
       "      <td>2.965103e+05</td>\n",
       "      <td>73609.057536</td>\n",
       "      <td>8.430289e+05</td>\n",
       "      <td>9905.938130</td>\n",
       "      <td>234890.291193</td>\n",
       "      <td>6.946113e+05</td>\n",
       "      <td>44091.548613</td>\n",
       "      <td>5253.565829</td>\n",
       "      <td>2.060869e+06</td>\n",
       "      <td>4.312171e+05</td>\n",
       "      <td>6.804777e+05</td>\n",
       "      <td>3.642363e+05</td>\n",
       "      <td>1.213942e+06</td>\n",
       "      <td>2.417881e+06</td>\n",
       "      <td>9.594110e+05</td>\n",
       "      <td>2.859655e+06</td>\n",
       "      <td>1.167976e+06</td>\n",
       "      <td>1.936994e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11 12:00:00</th>\n",
       "      <td>3.338677e+07</td>\n",
       "      <td>8.930125e+05</td>\n",
       "      <td>197514.376161</td>\n",
       "      <td>1.616912e+06</td>\n",
       "      <td>26623.483445</td>\n",
       "      <td>358801.206634</td>\n",
       "      <td>9.011416e+05</td>\n",
       "      <td>50717.331631</td>\n",
       "      <td>87151.192491</td>\n",
       "      <td>4.607640e+06</td>\n",
       "      <td>4.146479e+05</td>\n",
       "      <td>9.643264e+05</td>\n",
       "      <td>1.531237e+06</td>\n",
       "      <td>2.139694e+06</td>\n",
       "      <td>3.088998e+06</td>\n",
       "      <td>1.936027e+06</td>\n",
       "      <td>5.396577e+06</td>\n",
       "      <td>2.372525e+06</td>\n",
       "      <td>3.687622e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11 13:00:00</th>\n",
       "      <td>2.599866e+07</td>\n",
       "      <td>4.148004e+05</td>\n",
       "      <td>198932.907884</td>\n",
       "      <td>1.686487e+06</td>\n",
       "      <td>59221.774114</td>\n",
       "      <td>374679.172660</td>\n",
       "      <td>4.930206e+05</td>\n",
       "      <td>26798.314578</td>\n",
       "      <td>14804.652222</td>\n",
       "      <td>2.158669e+06</td>\n",
       "      <td>3.196139e+05</td>\n",
       "      <td>1.138288e+06</td>\n",
       "      <td>4.748964e+05</td>\n",
       "      <td>9.989755e+05</td>\n",
       "      <td>3.447916e+06</td>\n",
       "      <td>6.709845e+05</td>\n",
       "      <td>3.749217e+06</td>\n",
       "      <td>1.116360e+06</td>\n",
       "      <td>1.535185e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11 14:00:00</th>\n",
       "      <td>2.316263e+07</td>\n",
       "      <td>4.656195e+05</td>\n",
       "      <td>202794.197560</td>\n",
       "      <td>1.546374e+06</td>\n",
       "      <td>65825.513191</td>\n",
       "      <td>500245.935657</td>\n",
       "      <td>8.663746e+05</td>\n",
       "      <td>41735.371644</td>\n",
       "      <td>51167.955798</td>\n",
       "      <td>2.911821e+06</td>\n",
       "      <td>1.174731e+06</td>\n",
       "      <td>6.018775e+05</td>\n",
       "      <td>7.514082e+05</td>\n",
       "      <td>8.436561e+05</td>\n",
       "      <td>2.235798e+06</td>\n",
       "      <td>2.164486e+06</td>\n",
       "      <td>2.927404e+06</td>\n",
       "      <td>1.021063e+06</td>\n",
       "      <td>4.933233e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11 15:00:00</th>\n",
       "      <td>1.030635e+08</td>\n",
       "      <td>1.905839e+06</td>\n",
       "      <td>883489.368305</td>\n",
       "      <td>1.180470e+07</td>\n",
       "      <td>67224.749233</td>\n",
       "      <td>606386.051950</td>\n",
       "      <td>2.321021e+06</td>\n",
       "      <td>86537.896953</td>\n",
       "      <td>23354.055154</td>\n",
       "      <td>7.685622e+06</td>\n",
       "      <td>2.388541e+06</td>\n",
       "      <td>4.206329e+06</td>\n",
       "      <td>1.038961e+06</td>\n",
       "      <td>7.273102e+06</td>\n",
       "      <td>2.777626e+06</td>\n",
       "      <td>4.591357e+06</td>\n",
       "      <td>2.171652e+07</td>\n",
       "      <td>5.179136e+06</td>\n",
       "      <td>1.022772e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              BTC           ADA            XMR           EOS  \\\n",
       "group                                                                          \n",
       "2020-01-01 00:00:00  3.675857e+06  3.818918e+04   45395.983051  9.477858e+04   \n",
       "2020-01-01 01:00:00  6.365953e+06  5.135701e+04   33483.951528  5.932921e+05   \n",
       "2020-01-01 02:00:00  4.736719e+06  3.616426e+04   15732.553860  2.667326e+05   \n",
       "2020-01-01 03:00:00  5.667367e+06  2.444995e+04   25751.054662  1.245166e+05   \n",
       "2020-01-01 04:00:00  3.379094e+06  4.450267e+04   62955.628253  4.218197e+05   \n",
       "...                           ...           ...            ...           ...   \n",
       "2020-02-11 11:00:00  1.438900e+07  2.965103e+05   73609.057536  8.430289e+05   \n",
       "2020-02-11 12:00:00  3.338677e+07  8.930125e+05  197514.376161  1.616912e+06   \n",
       "2020-02-11 13:00:00  2.599866e+07  4.148004e+05  198932.907884  1.686487e+06   \n",
       "2020-02-11 14:00:00  2.316263e+07  4.656195e+05  202794.197560  1.546374e+06   \n",
       "2020-02-11 15:00:00  1.030635e+08  1.905839e+06  883489.368305  1.180470e+07   \n",
       "\n",
       "                              CHZ          MATIC           TRX           ENJ  \\\n",
       "group                                                                          \n",
       "2020-01-01 00:00:00    817.146319   31003.791035  4.819934e+05  15241.945783   \n",
       "2020-01-01 01:00:00    886.460339   84465.335718  5.336686e+05  11896.843688   \n",
       "2020-01-01 02:00:00   1819.795050  113379.718506  3.870500e+05  30109.770521   \n",
       "2020-01-01 03:00:00   2979.655803   41771.707995  4.507721e+05   6732.833578   \n",
       "2020-01-01 04:00:00   1023.388675   22254.756114  2.847890e+05    846.938455   \n",
       "...                           ...            ...           ...           ...   \n",
       "2020-02-11 11:00:00   9905.938130  234890.291193  6.946113e+05  44091.548613   \n",
       "2020-02-11 12:00:00  26623.483445  358801.206634  9.011416e+05  50717.331631   \n",
       "2020-02-11 13:00:00  59221.774114  374679.172660  4.930206e+05  26798.314578   \n",
       "2020-02-11 14:00:00  65825.513191  500245.935657  8.663746e+05  41735.371644   \n",
       "2020-02-11 15:00:00  67224.749233  606386.051950  2.321021e+06  86537.896953   \n",
       "\n",
       "                              FTM           BNB           XLM          BUSD  \\\n",
       "group                                                                         \n",
       "2020-01-01 00:00:00   1165.788613  8.498617e+05  9.460820e+03  1.352376e+04   \n",
       "2020-01-01 01:00:00    413.844612  7.405759e+05  3.714191e+04  2.531605e+04   \n",
       "2020-01-01 02:00:00   3559.965968  1.039091e+06  1.687882e+04  1.390886e+04   \n",
       "2020-01-01 03:00:00   4076.415482  4.975018e+05  9.049223e+03  2.251969e+04   \n",
       "2020-01-01 04:00:00    633.367505  4.751285e+05  7.254260e+03  1.122460e+04   \n",
       "...                           ...           ...           ...           ...   \n",
       "2020-02-11 11:00:00   5253.565829  2.060869e+06  4.312171e+05  6.804777e+05   \n",
       "2020-02-11 12:00:00  87151.192491  4.607640e+06  4.146479e+05  9.643264e+05   \n",
       "2020-02-11 13:00:00  14804.652222  2.158669e+06  3.196139e+05  1.138288e+06   \n",
       "2020-02-11 14:00:00  51167.955798  2.911821e+06  1.174731e+06  6.018775e+05   \n",
       "2020-02-11 15:00:00  23354.055154  7.685622e+06  2.388541e+06  4.206329e+06   \n",
       "\n",
       "                             ATOM           LTC          LINK           ETC  \\\n",
       "group                                                                         \n",
       "2020-01-01 00:00:00  3.198697e+04  1.165827e+05  2.428117e+04  5.648840e+04   \n",
       "2020-01-01 01:00:00  8.177767e+04  2.830715e+05  5.119098e+04  1.821021e+05   \n",
       "2020-01-01 02:00:00  1.957312e+05  2.402871e+05  2.872176e+04  1.340634e+05   \n",
       "2020-01-01 03:00:00  1.201133e+05  1.613043e+05  2.959622e+04  1.310942e+05   \n",
       "2020-01-01 04:00:00  1.998917e+04  2.214516e+05  5.451437e+04  1.349371e+05   \n",
       "...                           ...           ...           ...           ...   \n",
       "2020-02-11 11:00:00  3.642363e+05  1.213942e+06  2.417881e+06  9.594110e+05   \n",
       "2020-02-11 12:00:00  1.531237e+06  2.139694e+06  3.088998e+06  1.936027e+06   \n",
       "2020-02-11 13:00:00  4.748964e+05  9.989755e+05  3.447916e+06  6.709845e+05   \n",
       "2020-02-11 14:00:00  7.514082e+05  8.436561e+05  2.235798e+06  2.164486e+06   \n",
       "2020-02-11 15:00:00  1.038961e+06  7.273102e+06  2.777626e+06  4.591357e+06   \n",
       "\n",
       "                              ETH           XRP           BCH  \n",
       "group                                                          \n",
       "2020-01-01 00:00:00  1.000930e+06  2.579254e+05  1.782587e+05  \n",
       "2020-01-01 01:00:00  1.474278e+06  4.520609e+05  6.153210e+05  \n",
       "2020-01-01 02:00:00  9.940256e+05  4.414948e+05  2.215356e+05  \n",
       "2020-01-01 03:00:00  6.473610e+05  1.886061e+05  3.971860e+05  \n",
       "2020-01-01 04:00:00  4.430067e+05  2.279373e+05  3.164991e+05  \n",
       "...                           ...           ...           ...  \n",
       "2020-02-11 11:00:00  2.859655e+06  1.167976e+06  1.936994e+06  \n",
       "2020-02-11 12:00:00  5.396577e+06  2.372525e+06  3.687622e+06  \n",
       "2020-02-11 13:00:00  3.749217e+06  1.116360e+06  1.535185e+06  \n",
       "2020-02-11 14:00:00  2.927404e+06  1.021063e+06  4.933233e+06  \n",
       "2020-02-11 15:00:00  2.171652e+07  5.179136e+06  1.022772e+07  \n",
       "\n",
       "[1000 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_parquet('data.parquet')\n",
    "df = df[(df.index >= pd.Timestamp('2020-01-01')) & (df.index < pd.Timestamp('2023-01-01'))]\n",
    "assets = ['BTC', 'ETH', 'ADA', 'XMR', 'EOS', 'MATIC', 'TRX', 'FTM', 'BNB', 'XLM', 'ENJ', 'CHZ', 'BUSD', 'ATOM', 'LINK', 'ETC', 'XRP', 'BCH', 'LTC']\n",
    "df = df[[c for c in df.columns if 'quote asset volume' in c and any(asset in c for asset in assets)]]\n",
    "df.columns = [c.replace(' quote asset volume', '') for c in df.columns]\n",
    "df = df.head(1000)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b0b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    def __init__(self, feature_axis=None, minmax_range=(0, 1)):\n",
    "        \"\"\"\n",
    "        Initialize the MinMaxScaler.\n",
    "        Args:\n",
    "        feature_axis (int, optional): The axis that represents the feature dimension if applicable.\n",
    "                                      Use only for 3D data to specify which axis is the feature axis.\n",
    "                                      Default is None, automatically managed based on data dimensions.\n",
    "        \"\"\"\n",
    "        self.feature_axis = feature_axis\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "        self.scale_ = None\n",
    "        self.minmax_range = minmax_range # Default range for scaling (min, max)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the scaler to the data based on its dimensionality.\n",
    "        Args:\n",
    "        X (np.array): The data to fit the scaler on.\n",
    "        \"\"\"\n",
    "        if X.ndim == 3 and self.feature_axis is not None:  # 3D data\n",
    "            axis = tuple(i for i in range(X.ndim) if i != self.feature_axis)\n",
    "            self.min_ = np.min(X, axis=axis)\n",
    "            self.max_ = np.max(X, axis=axis)\n",
    "        elif X.ndim == 2:  # 2D data\n",
    "            self.min_ = np.min(X, axis=0)\n",
    "            self.max_ = np.max(X, axis=0)\n",
    "        elif X.ndim == 1:  # 1D data\n",
    "            self.min_ = np.min(X)\n",
    "            self.max_ = np.max(X)\n",
    "        else:\n",
    "            raise ValueError(\"Data must be 1D, 2D, or 3D.\")\n",
    "\n",
    "        self.scale_ = self.max_ - self.min_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data using the fitted scaler.\n",
    "        Args:\n",
    "        X (np.array): The data to transform.\n",
    "        Returns:\n",
    "        np.array: The scaled data.\n",
    "        \"\"\"\n",
    "        X_scaled = (X - self.min_) / self.scale_\n",
    "        X_scaled = X_scaled * (self.minmax_range[1] - self.minmax_range[0]) + self.minmax_range[0]\n",
    "        return X_scaled\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        Args:\n",
    "        X (np.array): The data to fit and transform.\n",
    "        Returns:\n",
    "        np.array: The scaled data.\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        Inverse transform the scaled data to original data.\n",
    "        Args:\n",
    "        X_scaled (np.array): The scaled data to inverse transform.\n",
    "        Returns:\n",
    "        np.array: The original data scale.\n",
    "        \"\"\"\n",
    "        X = (X_scaled - self.minmax_range[0]) / (self.minmax_range[1] - self.minmax_range[0])\n",
    "        X = X * self.scale_ + self.min_\n",
    "        return X\n",
    "\n",
    "def generate_data(df, sequence_length, n_ahead = 1):\n",
    "    #Case without known inputs\n",
    "    scaler_df = df.copy().shift(n_ahead).rolling(24 * 14).median()\n",
    "    tmp_df = df.copy() / scaler_df\n",
    "    tmp_df = tmp_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    scaler_df = scaler_df.iloc[24 * 14 + n_ahead:].fillna(0.)\n",
    "    def prepare_sequences(df, scaler_df, n_history, n_future):\n",
    "        X, y, y_scaler = [], [], []\n",
    "        num_features = df.shape[1]\n",
    "        \n",
    "        # Iterate through the DataFrame to create sequences\n",
    "        for i in range(n_history, len(df) - n_future + 1):\n",
    "            # Extract the sequence of past observations\n",
    "            X.append(df.iloc[i - n_history:i].values)\n",
    "            # Extract the future values of the first column\n",
    "            y.append(df.iloc[i:i + n_future,0:1].values)\n",
    "            y_scaler.append(scaler_df.iloc[i:i + n_future,0:1].values)\n",
    "        \n",
    "        X, y, y_scaler = np.array(X), np.array(y), np.array(y_scaler)\n",
    "        return X, y, y_scaler\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X, y, y_scaler = prepare_sequences(tmp_df, scaler_df, sequence_length, n_ahead)\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_test_separation = int(len(X) * 0.8)\n",
    "    X_train_unscaled, X_test_unscaled = X[:train_test_separation], X[train_test_separation:]\n",
    "    y_train_unscaled, y_test_unscaled = y[:train_test_separation], y[train_test_separation:]\n",
    "    y_scaler_train, y_scaler_test = y_scaler[:train_test_separation], y_scaler[train_test_separation:]\n",
    "    \n",
    "    # Generate the data\n",
    "    X_scaler = MinMaxScaler(feature_axis=2)\n",
    "    X_train = X_scaler.fit_transform(X_train_unscaled)\n",
    "    X_test = X_scaler.transform(X_test_unscaled)\n",
    "    \n",
    "    y_scaler = MinMaxScaler(feature_axis=2)\n",
    "    y_train = y_scaler.fit_transform(y_train_unscaled)\n",
    "    y_test = y_scaler.transform(y_test_unscaled)\n",
    "    \n",
    "    y_train = y_train.reshape(y_train.shape[0], -1) \n",
    "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "    return X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa901df",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "early_stopping_callback = lambda : tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00001,    \n",
    "    patience=6,         \n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=3, \n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "lr_callback = lambda : tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.25,\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.0001,  \n",
    "    min_lr=0.000025,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callbacks = lambda : [early_stopping_callback(), lr_callback(), tf.keras.callbacks.TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32621fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ n_ahead=1: X_train shape=(494, 45, 19), y_train shape=(494, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TKAN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TKAN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ tkan (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TKAN</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,534</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ tkan_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TKAN</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ tkan (\u001b[38;5;33mTKAN\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m41,534\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ tkan_1 (\u001b[38;5;33mTKAN\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m67,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,535</span> (427.87 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,535\u001b[0m (427.87 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,145</span> (426.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,145\u001b[0m (426.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> (1.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m390\u001b[0m (1.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_14900\\1143613624.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     73\u001b[39m                 model.summary()\n\u001b[32m     74\u001b[39m \n\u001b[32m     75\u001b[39m             \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m     76\u001b[39m             start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m             history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=N_MAX_EPOCHS, validation_split=\u001b[32m0.2\u001b[39m, callbacks=callbacks(), shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     78\u001b[39m             end_time = time.time()\n\u001b[32m     79\u001b[39m             time_results[model_id][n_ahead].append(end_time - start_time)\n\u001b[32m     80\u001b[39m             \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    373\u001b[39m             callbacks.on_epoch_begin(epoch)\n\u001b[32m    374\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m    375\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;28;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m                     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m                     logs = self.train_function(iterator)\n\u001b[32m    378\u001b[39m                     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m self.stop_training:\n\u001b[32m    380\u001b[39m                         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m function(iterator):\n\u001b[32m    217\u001b[39m             if isinstance(\n\u001b[32m    218\u001b[39m                 iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m             ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m                 opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m    221\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m StopIteration\n\u001b[32m    223\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    806\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, *args, **kwds):\n\u001b[32m    807\u001b[39m     \u001b[38;5;66;03m# Implements PolymorphicFunction.__call__.\u001b[39;00m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m self._run_functions_eagerly:\n\u001b[32m    809\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(self._name, tf_function_call=\u001b[33m\"eager\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._python_function(*args, **kwds)\n\u001b[32m    811\u001b[39m \n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m     \u001b[38;5;66;03m# place.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    129\u001b[39m         @tf.autograph.experimental.do_not_convert\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m multi_step_on_iterator(iterator):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    132\u001b[39m                 return tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m                     one_step_on_data(iterator.get_next())\n\u001b[32m    134\u001b[39m                 )\n\u001b[32m    135\u001b[39m \n\u001b[32m    136\u001b[39m             \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    806\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, *args, **kwds):\n\u001b[32m    807\u001b[39m     \u001b[38;5;66;03m# Implements PolymorphicFunction.__call__.\u001b[39;00m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m self._run_functions_eagerly:\n\u001b[32m    809\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(self._name, tf_function_call=\u001b[33m\"eager\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._python_function(*args, **kwds)\n\u001b[32m    811\u001b[39m \n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m     \u001b[38;5;66;03m# place.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m         @tf.autograph.experimental.do_not_convert\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m one_step_on_data(data):\n\u001b[32m    113\u001b[39m             \u001b[33m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m             outputs = self.distribute_strategy.run(step_function, args=(data,))\n\u001b[32m    115\u001b[39m             outputs = reduce_per_replica(\n\u001b[32m    116\u001b[39m                 outputs,\n\u001b[32m    117\u001b[39m                 self.distribute_strategy,\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1669\u001b[39m       \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m       \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3259\u001b[39m     _require_cross_replica_or_default_context_extended(self)\n\u001b[32m   3260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3261\u001b[39m       kwargs = {}\n\u001b[32m   3262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self._call_for_each_replica(fn, args, kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _call_for_each_replica(self, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(self._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m     57\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self._call_has_training_arg:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m                 y_pred = self(x, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m                 y_pred = self(x)\n\u001b[32m     61\u001b[39m             loss = self._compute_loss(\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    970\u001b[39m                     \u001b[33m\"layers will not see the mask.\"\u001b[39m\n\u001b[32m    971\u001b[39m                 )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    973\u001b[39m             \u001b[38;5;66;03m# Destroy call context if we created it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m             self._maybe_reset_call_context()\n\u001b[32m    975\u001b[39m \n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m################################################\u001b[39;00m\n\u001b[32m    977\u001b[39m         \u001b[38;5;66;03m# 8. Add a node in the graph for symbolic calls.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     55\u001b[39m                 call_fn,\n\u001b[32m     56\u001b[39m                 object_name=(\u001b[33mf\"{self.__class__.__name__}.call()\"\u001b[39m),\n\u001b[32m     57\u001b[39m             )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(*args, **kwargs)\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m                 new_e = e\n\u001b[32m    214\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m new_e.with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    216\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m signature\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m bound_signature\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\models\\sequential.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m call(self, inputs, training=\u001b[38;5;28;01mNone\u001b[39;00m, mask=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._functional:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m             return self._functional.call(\n\u001b[32m    221\u001b[39m                 inputs, training=training, mask=mask, **kwargs\n\u001b[32m    222\u001b[39m             )\n\u001b[32m    223\u001b[39m \n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m             masks = tree.flatten(mask)\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m x, mask \u001b[38;5;28;01min\u001b[39;00m zip(inputs, masks):\n\u001b[32m    181\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m                     backend.set_keras_mask(x, mask)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m         outputs = self._run_through_graph(\n\u001b[32m    184\u001b[39m             inputs,\n\u001b[32m    185\u001b[39m             operation_fn=lambda op: operation_fn(\n\u001b[32m    186\u001b[39m                 op, training=training, **kwargs\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\ops\\function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    173\u001b[39m                 op = operation_fn(node.operation)\n\u001b[32m    174\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m call_fn \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    175\u001b[39m                     outputs = call_fn(op, *args, **kwargs)\n\u001b[32m    176\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m                     outputs = op(*args, **kwargs)\n\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m                 \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    180\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;28;01min\u001b[39;00m zip(node.outputs, tree.flatten(outputs)):\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    644\u001b[39m                 \u001b[38;5;28;01mand\u001b[39;00m value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    645\u001b[39m             ):\n\u001b[32m    646\u001b[39m                 kwargs[name] = value\n\u001b[32m    647\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m operation(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    970\u001b[39m                     \u001b[33m\"layers will not see the mask.\"\u001b[39m\n\u001b[32m    971\u001b[39m                 )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    973\u001b[39m             \u001b[38;5;66;03m# Destroy call context if we created it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m             self._maybe_reset_call_context()\n\u001b[32m    975\u001b[39m \n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m################################################\u001b[39;00m\n\u001b[32m    977\u001b[39m         \u001b[38;5;66;03m# 8. Add a node in the graph for symbolic calls.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     55\u001b[39m                 call_fn,\n\u001b[32m     56\u001b[39m                 object_name=(\u001b[33mf\"{self.__class__.__name__}.call()\"\u001b[39m),\n\u001b[32m     57\u001b[39m             )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(*args, **kwargs)\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m                 new_e = e\n\u001b[32m    214\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m new_e.with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    216\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m signature\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m bound_signature\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\DeepLearning\\DeepLearning\\Projet\\sigtkan\\tkan.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m call(self, sequences, initial_state=\u001b[38;5;28;01mNone\u001b[39;00m, mask=\u001b[38;5;28;01mNone\u001b[39;00m, training=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m         return super().call(\n\u001b[32m    470\u001b[39m             sequences, mask=mask, training=training, initial_state=initial_state\n\u001b[32m    471\u001b[39m         )\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    402\u001b[39m         self._maybe_config_dropout_masks(\n\u001b[32m    403\u001b[39m             self.cell, sequences[:, \u001b[32m0\u001b[39m, :], initial_state\n\u001b[32m    404\u001b[39m         )\n\u001b[32m    405\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         last_output, outputs, states = self.inner_loop(\n\u001b[32m    407\u001b[39m             sequences=sequences,\n\u001b[32m    408\u001b[39m             initial_state=initial_state,\n\u001b[32m    409\u001b[39m             mask=mask,\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\DeepLearning\\DeepLearning\\Projet\\sigtkan\\tkan.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m inner_loop(self, sequences, initial_state, mask, training=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    462\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(mask, (list, tuple)):\n\u001b[32m    463\u001b[39m             mask = mask[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         return super().inner_loop(\n\u001b[32m    465\u001b[39m             sequences, initial_state, mask=mask, training=training\n\u001b[32m    466\u001b[39m         )\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    342\u001b[39m \n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m tree.is_nested(initial_state):\n\u001b[32m    344\u001b[39m             initial_state = [initial_state]\n\u001b[32m    345\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m         return backend.rnn(\n\u001b[32m    347\u001b[39m             step,\n\u001b[32m    348\u001b[39m             sequences,\n\u001b[32m    349\u001b[39m             initial_state,\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[39m\n\u001b[32m    424\u001b[39m                     initial_states, flat_new_state\n\u001b[32m    425\u001b[39m                 )\n\u001b[32m    426\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m (time + \u001b[32m1\u001b[39m, output_ta_t) + tuple(new_states)\n\u001b[32m    427\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m             final_outputs = tf.while_loop(\n\u001b[32m    429\u001b[39m                 body=_step,\n\u001b[32m    430\u001b[39m                 loop_vars=(time, output_ta) + states,\n\u001b[32m    431\u001b[39m                 **while_loop_kwargs,\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    656\u001b[39m                   _call_location(), decorator_utils.get_qualified_name(func),\n\u001b[32m    657\u001b[39m                   func.__module__, arg_name, arg_value,\n\u001b[32m    658\u001b[39m                   \u001b[33m'in a future version'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m    659\u001b[39m                   ('after %s' % date), instructions)\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[39m\n\u001b[32m    237\u001b[39m   ...   print(sess.run(x_out).shape)\n\u001b[32m    238\u001b[39m   (\u001b[32m1000\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m    239\u001b[39m \n\u001b[32m    240\u001b[39m   \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m   return while_loop(\n\u001b[32m    242\u001b[39m       cond=cond,\n\u001b[32m    243\u001b[39m       body=body,\n\u001b[32m    244\u001b[39m       loop_vars=loop_vars,\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[39m\n\u001b[32m    484\u001b[39m \n\u001b[32m    485\u001b[39m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[32m    486\u001b[39m                                               list(loop_vars))\n\u001b[32m    487\u001b[39m       \u001b[38;5;28;01mwhile\u001b[39;00m cond(*loop_vars):\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         loop_vars = body(*loop_vars)\n\u001b[32m    489\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(loop_vars, (list, tuple)):\n\u001b[32m    490\u001b[39m           packed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    491\u001b[39m           loop_vars = (loop_vars,)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(i, lv)\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         body = \u001b[38;5;28;01mlambda\u001b[39;00m i, lv: (i + \u001b[32m1\u001b[39m, orig_body(*lv))\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(time, output_ta_t, *states)\u001b[39m\n\u001b[32m    407\u001b[39m                     Tuple: `(time + \u001b[32m1\u001b[39m,output_ta_t) + tuple(new_states)`\n\u001b[32m    408\u001b[39m                 \"\"\"\n\u001b[32m    409\u001b[39m                 current_input = tuple(ta.read(time) \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;28;01min\u001b[39;00m input_ta)\n\u001b[32m    410\u001b[39m                 current_input = tree.pack_sequence_as(inputs, current_input)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m                 output, new_states = step_function(\n\u001b[32m    412\u001b[39m                     current_input, tuple(states) + tuple(constants)\n\u001b[32m    413\u001b[39m                 )\n\u001b[32m    414\u001b[39m                 flat_new_state = tree.flatten(new_states)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(inputs, states)\u001b[39m\n\u001b[32m    334\u001b[39m             \u001b[38;5;66;03m# that would otherwise break PyTorch's autograd functionality\u001b[39;00m\n\u001b[32m    335\u001b[39m             \u001b[38;5;66;03m# by modifying tensors needed for gradient computation.\u001b[39;00m\n\u001b[32m    336\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m backend.backend() == \u001b[33m\"torch\"\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m self.stateful:\n\u001b[32m    337\u001b[39m                 states = tree.map_structure(ops.copy, states)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m             output, new_states = self.cell(inputs, states, **cell_kwargs)\n\u001b[32m    339\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m tree.is_nested(new_states):\n\u001b[32m    340\u001b[39m                 new_states = [new_states]\n\u001b[32m    341\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m output, new_states\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    970\u001b[39m                     \u001b[33m\"layers will not see the mask.\"\u001b[39m\n\u001b[32m    971\u001b[39m                 )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    973\u001b[39m             \u001b[38;5;66;03m# Destroy call context if we created it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m             self._maybe_reset_call_context()\n\u001b[32m    975\u001b[39m \n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m################################################\u001b[39;00m\n\u001b[32m    977\u001b[39m         \u001b[38;5;66;03m# 8. Add a node in the graph for symbolic calls.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     55\u001b[39m                 call_fn,\n\u001b[32m     56\u001b[39m                 object_name=(\u001b[33mf\"{self.__class__.__name__}.call()\"\u001b[39m),\n\u001b[32m     57\u001b[39m             )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(*args, **kwargs)\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m                 new_e = e\n\u001b[32m    214\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m new_e.with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    216\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m signature\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m bound_signature\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\DeepLearning\\DeepLearning\\Projet\\sigtkan\\tkan.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, states, training)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m call(self, inputs, states, training=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.backend == \u001b[33m'tensorflow'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._call_tensorflow(inputs, states, training)\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._call_generic(inputs, states, training)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\DeepLearning\\DeepLearning\\Projet\\sigtkan\\tkan.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, states, training)\u001b[39m\n\u001b[32m    287\u001b[39m         \u001b[38;5;66;03m# Process each sub-layer\u001b[39;00m\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (sub_layer, sub_state) \u001b[38;5;28;01min\u001b[39;00m enumerate(zip(self.tkan_sub_layers, sub_states)):\n\u001b[32m    289\u001b[39m             sub_kernel_x, sub_kernel_h = self.sub_tkan_recurrent_kernel_inputs[idx], self.sub_tkan_recurrent_kernel_states[idx]\n\u001b[32m    290\u001b[39m             agg_input = inputs @ sub_kernel_x + sub_state @ sub_kernel_h\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m             sub_output = sub_layer(agg_input)\n\u001b[32m    292\u001b[39m             sub_recurrent_kernel_h, sub_recurrent_kernel_x = tf.split(self.sub_tkan_kernel[idx], \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    293\u001b[39m             new_sub_state = sub_recurrent_kernel_h * sub_output + sub_state * sub_recurrent_kernel_x\n\u001b[32m    294\u001b[39m \n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    970\u001b[39m                     \u001b[33m\"layers will not see the mask.\"\u001b[39m\n\u001b[32m    971\u001b[39m                 )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    973\u001b[39m             \u001b[38;5;66;03m# Destroy call context if we created it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m             self._maybe_reset_call_context()\n\u001b[32m    975\u001b[39m \n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m################################################\u001b[39;00m\n\u001b[32m    977\u001b[39m         \u001b[38;5;66;03m# 8. Add a node in the graph for symbolic calls.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     55\u001b[39m                 call_fn,\n\u001b[32m     56\u001b[39m                 object_name=(\u001b[33mf\"{self.__class__.__name__}.call()\"\u001b[39m),\n\u001b[32m     57\u001b[39m             )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(*args, **kwargs)\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m                 new_e = e\n\u001b[32m    214\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m new_e.with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    216\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m signature\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m bound_signature\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras_efficient_kan\\kan.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x, training)\u001b[39m\n\u001b[32m    119\u001b[39m         base_output = ops.matmul(base_activation(x_2d), self.base_weight)\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.use_bias:\n\u001b[32m    121\u001b[39m             base_output = ops.add(base_output, self.base_bias)\n\u001b[32m    122\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         spline_output = ops.matmul(self.b_splines(x_2d), self.spline_weight)\n\u001b[32m    124\u001b[39m         output_2d = self.dropout(base_output, training=training) + self.dropout(spline_output, training=training)\n\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m         \u001b[38;5;66;03m# Use ops.reshape with a tuple of integers for the new shape\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\keras_efficient_kan\\kan.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m1\u001b[39m, self.spline_order + \u001b[32m1\u001b[39m):\n\u001b[32m    135\u001b[39m             left_denominator = self.grid[..., k:-\u001b[32m1\u001b[39m] - self.grid[..., :-(k + \u001b[32m1\u001b[39m)]\n\u001b[32m    136\u001b[39m             right_denominator = self.grid[..., k + \u001b[32m1\u001b[39m:] - self.grid[..., \u001b[32m1\u001b[39m:-k]\n\u001b[32m    137\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             left = (x_expanded - self.grid[..., :-(k + \u001b[32m1\u001b[39m)]) / left_denominator\n\u001b[32m    139\u001b[39m             right = (self.grid[..., k + \u001b[32m1\u001b[39m:] - x_expanded) / right_denominator\n\u001b[32m    140\u001b[39m             bases = left * bases[..., :-\u001b[32m1\u001b[39m] + right * bases[..., \u001b[32m1\u001b[39m:]\n\u001b[32m    141\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ops.reshape(bases, [ops.shape(x)[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m])\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m    110\u001b[39m         \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[32m    111\u001b[39m         \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[32m    112\u001b[39m         x, y = maybe_promote_tensors(x, y)\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name=name)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    115\u001b[39m         \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[32m    116\u001b[39m         \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[32m    117\u001b[39m         \u001b[38;5;66;03m# and the tensor.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m _subtract_factory(x, y, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     92\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m tensorflow.python.ops \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[32m     93\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m math_ops.subtract(x, y, name=name)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    541\u001b[39m @tf_export(\u001b[33m\"math.subtract\"\u001b[39m, \u001b[33m\"subtract\"\u001b[39m)\n\u001b[32m    542\u001b[39m @dispatch.register_binary_elementwise_api\n\u001b[32m    543\u001b[39m @dispatch.add_dispatch_support\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m subtract(x, y, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.sub(x, y, name)\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32mc:\\Users\\tmont\\Documents\\Cours\\Dauphine\\Machine Learning\\ml_project-1\\.conda\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m  12304\u001b[39m         _ctx, \"Sub\", name, x, y)\n\u001b[32m  12305\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m  12306\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m  12307\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m> \u001b[39m\u001b[32m12308\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m  12309\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  12310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  12311\u001b[39m       return sub_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_aheads = [1, 3, 6, 9, 12, 15]\n",
    "models = [\n",
    "    \"TKAN\",\n",
    "    \"GRU\", \n",
    "    \"LSTM\",\n",
    "    \"SigTKAN\"\n",
    " ]\n",
    "\n",
    "results = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "results_rmse = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "time_results = {model: {n_ahead: [] for n_ahead in n_aheads} for model in models}\n",
    "\n",
    "for n_ahead in n_aheads:\n",
    "    sequence_length = max(45, 5 * n_ahead)\n",
    "    \n",
    "    try:\n",
    "        # Utilisation de la fonction generate_data\n",
    "        X_scaler, X_train, X_test, X_train_unscaled, X_test_unscaled, y_scaler, y_train, y_test, y_train_unscaled, y_test_unscaled, y_scaler_train, y_scaler_test = generate_data(df, sequence_length, n_ahead)\n",
    "        \n",
    "        # Vérifier que les données ne sont pas vides\n",
    "        if X_train.size == 0 or y_train.size == 0:\n",
    "            print(f\"⚠️ Données vides pour n_ahead={n_ahead}, sequence_length={sequence_length}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"✅ n_ahead={n_ahead}: X_train shape={X_train.shape}, y_train shape={y_train.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur génération données pour n_ahead={n_ahead}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # IMPORTANT: Cette partie doit être dans le try, pas après !\n",
    "    \n",
    "    # Maintenant les boucles de modèles sont dans le bon scope\n",
    "    for model_id in models:\n",
    "        \n",
    "        for run in range(10):\n",
    "\n",
    "            if model_id == 'TKAN':\n",
    "                model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    TKAN(100, return_sequences=True),\n",
    "                    TKAN(100, sub_kan_output_dim = 20, sub_kan_input_dim = 20, return_sequences=False),\n",
    "                    Dense(units=n_ahead, activation='linear')\n",
    "                ], name = model_id)\n",
    "            elif model_id == 'GRU':\n",
    "                model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    GRU(100, return_sequences=True),\n",
    "                    GRU(100, return_sequences=False),\n",
    "                    Dense(units=n_ahead, activation='linear')\n",
    "                ], name = model_id)\n",
    "            elif model_id == 'LSTM':\n",
    "                model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    LSTM(100, return_sequences=True),\n",
    "                    LSTM(100, return_sequences=False),\n",
    "                    Dense(units=n_ahead, activation='linear')\n",
    "                ], name = model_id)\n",
    "            elif model_id == 'SigTKAN':\n",
    "                model = Sequential([\n",
    "                    Input(shape=X_train.shape[1:]),\n",
    "                    SigTKAN(100, 2, dropout=0., return_sequences=True),\n",
    "                    SigTKAN(100, 2, dropout=0., return_sequences=False),\n",
    "                    Flatten(),\n",
    "                    Dense(units=n_ahead, activation='linear')\n",
    "                ], name = model_id)\n",
    "            else:\n",
    "                raise ValueError(f\"Model {model_id} not recognized\")\n",
    "            \n",
    "            optimizer = keras.optimizers.Adam(0.001)\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error', jit_compile=True)\n",
    "            if run==0:\n",
    "                model.summary()\n",
    "                \n",
    "            # Fit the model\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=N_MAX_EPOCHS, validation_split=0.2, callbacks=callbacks(), shuffle=True, verbose = False)\n",
    "            end_time = time.time()\n",
    "            time_results[model_id][n_ahead].append(end_time - start_time)\n",
    "            # Evaluate the model on the test set\n",
    "            preds = model.predict(X_test, verbose=False)\n",
    "            r2 = r2_score(y_true=y_test, y_pred=preds)\n",
    "            print(end_time - start_time, r2)\n",
    "            rmse = root_mean_squared_error(y_true=y_test, y_pred=preds)\n",
    "            results[model_id][n_ahead].append(r2)\n",
    "            results_rmse[model_id][n_ahead].append(rmse)\n",
    "    \n",
    "            del model\n",
    "            del optimizer\n",
    "                \n",
    "\n",
    "print('R2 scores')\n",
    "print('Means:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.mean(results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results.keys()}))\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.mean(results_rmse[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results_rmse.keys()}))\n",
    "print('Std:')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.std(results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results.keys()}))\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.std(results_rmse[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in results_rmse.keys()}))\n",
    "print('Training Times')\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.mean(time_results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in time_results.keys()}))\n",
    "display(pd.DataFrame({model_id: {n_ahead: np.std(time_results[model_id][n_ahead]) for n_ahead in n_aheads} for model_id in time_results.keys()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b4403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
